{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":918121,"sourceType":"datasetVersion","datasetId":494054}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport matplotlib.pyplot as plt\nimport random\nimport re\nimport string\nimport pickle\nfrom nltk.corpus import stopwords # module for stop words\nfrom nltk.stem import PorterStemmer # module for stemming\nfrom nltk.tokenize import TweetTokenizer # module for tokenizing strings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:38.374703Z","iopub.execute_input":"2025-07-01T02:07:38.375267Z","iopub.status.idle":"2025-07-01T02:07:43.445011Z","shell.execute_reply.started":"2025-07-01T02:07:38.375239Z","shell.execute_reply":"2025-07-01T02:07:43.444006Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# download the stop words\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:43.446835Z","iopub.execute_input":"2025-07-01T02:07:43.447413Z","iopub.status.idle":"2025-07-01T02:07:43.521039Z","shell.execute_reply.started":"2025-07-01T02:07:43.447379Z","shell.execute_reply":"2025-07-01T02:07:43.519943Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# 2. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Explore the dataset\nLook at the number of training dataset that is with label 1 (related to disaster) versus lable 0 (not related to disaster)","metadata":{}},{"cell_type":"code","source":"# import train dataset\ntrain_df = pd.read_csv(\"/kaggle/input/nlpgettingstarted/train.csv\")\n# check the number of dataset with label 1 and 0 \nprint(\"number of disaster training sample:\", len(train_df[train_df[\"target\"] == 1]))\nprint(\"number of not disaster training sample:\", len(train_df[train_df[\"target\"] == 0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:43.521993Z","iopub.execute_input":"2025-07-01T02:07:43.522268Z","iopub.status.idle":"2025-07-01T02:07:43.597648Z","shell.execute_reply.started":"2025-07-01T02:07:43.522247Z","shell.execute_reply":"2025-07-01T02:07:43.596692Z"}},"outputs":[{"name":"stdout","text":"number of disaster training sample: 3271\nnumber of not disaster training sample: 4342\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2.2. Data Cleaning and Tokenizing\n1. Remove the hash tag\n2. Remove hyperlink\n3. Remove any word that start with @\n4. Tokenize the text\n5. Remove stop words\n6. Remove punctuation\n7. Stemming","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer, which will make the string to be list and lowercase all the words\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n\n# Initialize stemmer, which will be used to stem the word\nstemmer = PorterStemmer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:43.598525Z","iopub.execute_input":"2025-07-01T02:07:43.598865Z","iopub.status.idle":"2025-07-01T02:07:43.603904Z","shell.execute_reply.started":"2025-07-01T02:07:43.598839Z","shell.execute_reply":"2025-07-01T02:07:43.602916Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def clean_tokenize(text):\n    \"\"\"Process text function.\n    Input:\n        text: the text of the tweet\n    Output:\n        clean_token: a list of words containing the processed tweet\n\n    \"\"\"\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    \n    # remove hyperlink\n    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n\n    # remove @\n    text = re.sub('@.*? ', '', text)\n\n    # Tokenize the text, which will also lowercase the word\n    text_token = tokenizer.tokenize(text)\n\n    # remove stop words, punctuation and stem the word\n    clean_token = []\n    for word in text_token:\n        if (word not in stopwords.words('english') and  # remove stopwords\n            word not in string.punctuation):   # remove punctuation\n            # stemming\n            clean_word = stemmer.stem(word)\n            clean_token.append(clean_word)\n    return clean_token\ntrain_df['Remove_Hash_Link_At'] = train_df['text'].map(clean_tokenize)\nprint(train_df.iloc[30:35])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:43.606330Z","iopub.execute_input":"2025-07-01T02:07:43.606681Z","iopub.status.idle":"2025-07-01T02:07:58.128177Z","shell.execute_reply.started":"2025-07-01T02:07:43.606656Z","shell.execute_reply":"2025-07-01T02:07:58.127070Z"}},"outputs":[{"name":"stdout","text":"    id keyword                       location  \\\n30  44     NaN                            NaN   \n31  48  ablaze                     Birmingham   \n32  49  ablaze  Est. September 2012 - Bristol   \n33  50  ablaze                         AFRICA   \n34  52  ablaze               Philadelphia, PA   \n\n                                                 text  target  \\\n30                                           The end!       0   \n31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n32  We always try to bring the heavy. #metal #RT h...       0   \n33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n34                 Crying out for more! Set me ablaze       0   \n\n                                  Remove_Hash_Link_At  \n30                                              [end]  \n31                          [wholesal, market, ablaz]  \n32              [alway, tri, bring, heavi, metal, rt]  \n33  [africanbaz, break, news, nigeria, flag, set, ...  \n34                                  [cri, set, ablaz]  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 3. Build Word Dictionary\nThe word dictionary will use the ($word_i$, $label_i$) as key and the count of ($word_i$, $label_i$) occurrence as value","metadata":{}},{"cell_type":"code","source":"def build_word_dict(label_arr, token_word_arr):\n    \"\"\"Build frequencies.\n    Input:\n        token_word: a series of list of tokenized word\n        label: a series of label that match the array of the list of tokenized word\n    Output:\n        freqs: a dictionary mapping each (word, label) pair to its frequency\n    \"\"\"\n    word_dict = {}\n    y_list = list(label_arr) # make array into list\n    for label_idx in range(len(y_list)):\n        for word in token_word_arr[label_idx]:\n            word_dict[(word, y_list[label_idx])] = word_dict.get((word, y_list[label_idx]), 0) + 1\n    return word_dict\n        \n    \n        \nword_dict = build_word_dict(train_df['target'], train_df['Remove_Hash_Link_At'])\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:58.129201Z","iopub.execute_input":"2025-07-01T02:07:58.129544Z","iopub.status.idle":"2025-07-01T02:07:58.189912Z","shell.execute_reply.started":"2025-07-01T02:07:58.129515Z","shell.execute_reply":"2025-07-01T02:07:58.189003Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(\"number of words:\", len(word_dict))\nprint(\"Output Example:\", list(word_dict.items())[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:58.190778Z","iopub.execute_input":"2025-07-01T02:07:58.191071Z","iopub.status.idle":"2025-07-01T02:07:58.199361Z","shell.execute_reply.started":"2025-07-01T02:07:58.191050Z","shell.execute_reply":"2025-07-01T02:07:58.198158Z"}},"outputs":[{"name":"stdout","text":"number of words: 15893\nOutput Example: [(('deed', 1), 1), (('reason', 1), 8), (('earthquak', 1), 47), (('may', 1), 50), (('allah', 1), 6), (('forgiv', 1), 1), (('us', 1), 49), (('forest', 1), 50), (('fire', 1), 271), (('near', 1), 49)]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# save the word dictionary\nwith open('/kaggle/working/word_dict.pickle', 'wb') as f:\n    pickle.dump(word_dict, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T02:07:58.200307Z","iopub.execute_input":"2025-07-01T02:07:58.200540Z","iopub.status.idle":"2025-07-01T02:07:58.230123Z","shell.execute_reply.started":"2025-07-01T02:07:58.200519Z","shell.execute_reply":"2025-07-01T02:07:58.228663Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Reference: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}