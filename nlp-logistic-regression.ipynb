{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":918121,"sourceType":"datasetVersion","datasetId":494054}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport matplotlib.pyplot as plt\nimport random\nimport re\nimport string\nimport pickle\nfrom nltk.corpus import stopwords # module for stop words\nfrom nltk.stem import PorterStemmer # module for stemming\nfrom nltk.tokenize import TweetTokenizer # module for tokenizing strings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:02.371229Z","iopub.execute_input":"2025-08-01T08:06:02.371649Z","iopub.status.idle":"2025-08-01T08:06:04.461481Z","shell.execute_reply.started":"2025-08-01T08:06:02.371615Z","shell.execute_reply":"2025-08-01T08:06:04.460632Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # download the stop words\n# nltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:04.462854Z","iopub.execute_input":"2025-08-01T08:06:04.463389Z","iopub.status.idle":"2025-08-01T08:06:04.467726Z","shell.execute_reply.started":"2025-08-01T08:06:04.463356Z","shell.execute_reply":"2025-08-01T08:06:04.466701Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 2. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Explore the dataset\nLook at the number of training dataset that is with label 1 (related to disaster) versus lable 0 (not related to disaster)","metadata":{}},{"cell_type":"code","source":"# import train dataset\nraw_df = pd.read_csv(\"/kaggle/input/nlpgettingstarted/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlpgettingstarted/test.csv\")\n# check the number of dataset with label 1 and 0 \nprint(\"number of disaster sample:\", len(raw_df[raw_df[\"target\"] == 1]))\nprint(\"number of not disaster sample:\", len(raw_df[raw_df[\"target\"] == 0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:07.349069Z","iopub.execute_input":"2025-08-01T08:06:07.349459Z","iopub.status.idle":"2025-08-01T08:06:07.468050Z","shell.execute_reply.started":"2025-08-01T08:06:07.349435Z","shell.execute_reply":"2025-08-01T08:06:07.466874Z"}},"outputs":[{"name":"stdout","text":"number of disaster sample: 3271\nnumber of not disaster sample: 4342\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2.2 Train/Validation Split\nSince the test dataset is given, I am creating the train and validation sets to find the best $\\theta$ that minimize the cost without overfitting\n* Train: 80% of disaster sample + 80% of not disaster sample\n* Validation: 20% of disaster sample + 20% of not disaster sample","metadata":{}},{"cell_type":"code","source":"# disaster sample\nX_train_dis, X_val_dis, y_train_dis, y_val_dis = train_test_split(raw_df[raw_df[\"target\"] == 1]['text'], \n                                                                    raw_df[raw_df[\"target\"] == 1]['target'],\n                                                                    test_size=0.20, random_state=42, shuffle=True)\n\n# not disaster sample \nX_train_ndis, X_val_ndis, y_train_ndis, y_val_ndis = train_test_split(raw_df[raw_df[\"target\"] == 0]['text'], \n                                                                    raw_df[raw_df[\"target\"] == 0]['target'],\n                                                                    test_size=0.20, random_state=42, shuffle=True)\nX_train = pd.concat([X_train_dis, X_train_ndis], axis = 0).reset_index(drop = True).to_frame()\ny_train = pd.concat([y_train_dis, y_train_ndis], axis = 0).reset_index(drop = True).to_frame()\ntrain_df = pd.concat([X_train, y_train], axis = 1)\ntrain_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle dataset\n\n\nX_val = pd.concat([X_val_dis, X_val_ndis], axis = 0).reset_index(drop = True).to_frame()\ny_val = pd.concat([y_val_dis, y_val_ndis], axis = 0).reset_index(drop = True).to_frame()\nval_df = pd.concat([X_val, y_val], axis = 1)\nval_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:07.967003Z","iopub.execute_input":"2025-08-01T08:06:07.967361Z","iopub.status.idle":"2025-08-01T08:06:07.992723Z","shell.execute_reply.started":"2025-08-01T08:06:07.967338Z","shell.execute_reply":"2025-08-01T08:06:07.991603Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:08.230986Z","iopub.execute_input":"2025-08-01T08:06:08.231372Z","iopub.status.idle":"2025-08-01T08:06:08.238646Z","shell.execute_reply.started":"2025-08-01T08:06:08.231348Z","shell.execute_reply":"2025-08-01T08:06:08.237557Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(6089, 2)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## 2.2. Data Cleaning and Tokenizing\n1. Remove the hash tag\n2. Remove hyperlink\n3. Remove any word that start with @\n4. Tokenize the text\n5. Remove stop words\n6. Remove punctuation\n7. Stemming","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer, which will make the string to be list and lowercase all the words\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n\n# Initialize stemmer, which will be used to stem the word\nstemmer = PorterStemmer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:10.361787Z","iopub.execute_input":"2025-08-01T08:06:10.362126Z","iopub.status.idle":"2025-08-01T08:06:10.369409Z","shell.execute_reply.started":"2025-08-01T08:06:10.362103Z","shell.execute_reply":"2025-08-01T08:06:10.367653Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def clean_tokenize(text):\n    \"\"\"Process text function.\n    Input:\n        text: the text of the tweet\n    Output:\n        clean_token: a list of words containing the processed tweet\n\n    \"\"\"\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    \n    # remove hyperlink\n    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n\n    # remove @\n    text = re.sub('@.*? ', '', text)\n\n    # remove ...\n    text = re.sub('\\.\\.\\.', '', text)\n\n    # remove \\x89\n    text = re.sub(r'\\x89', '', text)\n    \n    # remove could, would, should\n    text = re.sub(r'\\b(could|would|should)\\b', '', text, flags=re.IGNORECASE)\n\n    # Tokenize the text, which will also lowercase the word\n    text_token = tokenizer.tokenize(text)\n\n    # remove stop words, punctuation and stem the word\n    clean_token = []\n    for word in text_token:\n        if (word not in stopwords.words('english') and  # remove stopwords\n            word not in string.punctuation):   # remove punctuation\n            # stemming\n            clean_word = stemmer.stem(word)\n            # remove not readable words and digit\n            clean_word = re.sub(r'[^a-zA-Z]', ' ', clean_word)\n\n\n            clean_token.append(clean_word)\n    return clean_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:11.612648Z","iopub.execute_input":"2025-08-01T08:06:11.613108Z","iopub.status.idle":"2025-08-01T08:06:11.621662Z","shell.execute_reply.started":"2025-08-01T08:06:11.613076Z","shell.execute_reply":"2025-08-01T08:06:11.620438Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# clean training dataset\n\ntrain_df['Remove_Hash_Link_At'] = train_df['text'].map(clean_tokenize)\n\n# clean validation dataset\nval_df['Remove_Hash_Link_At'] = val_df['text'].map(clean_tokenize)\n\n# clean testing datast\ntest_df['Remove_Hash_Link_At'] = test_df['text'].map(clean_tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:13.064251Z","iopub.execute_input":"2025-08-01T08:06:13.064573Z","iopub.status.idle":"2025-08-01T08:06:34.254980Z","shell.execute_reply.started":"2025-08-01T08:06:13.064550Z","shell.execute_reply":"2025-08-01T08:06:34.253908Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Vanishing Gradient Return\n1. Apply TfidfVectorizer\n\n    a. TfidfVectorizer is a tool from scikit-learn that transforms a list of text documents into a matrix of TF-IDF features.\n\n   \n    b. TF-IDF = Term Frequency â€“ Inverse Document Frequency\n\n2. Scale the Dataset\n\n    a. Upon the first logistic regression training, I had discovered my model face the vanishing gradient return, where the gradient vector is too small for the model to update the $\\theta$ vector. To avoid this issue, I had decided to scale the data prior to training my logistic regression model.\n\n3. Apply PCA\n\n","metadata":{}},{"cell_type":"code","source":"# Combine the text to fit into the TfidVectorizer\ntrain_df[\"Remove_Hash_Link_At_Combine\"] = train_df['Remove_Hash_Link_At'].apply(lambda tokens: ' '.join(tokens))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:34.256538Z","iopub.execute_input":"2025-08-01T08:06:34.256915Z","iopub.status.idle":"2025-08-01T08:06:34.266035Z","shell.execute_reply.started":"2025-08-01T08:06:34.256884Z","shell.execute_reply":"2025-08-01T08:06:34.264956Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Apply TfidVectorizer\nvectorizer = TfidfVectorizer()\ntrain_tfidf = vectorizer.fit_transform(train_df[\"Remove_Hash_Link_At_Combine\"])\ntrain_tfidf = pd.DataFrame(train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\ntrain_tfidf.index = train_df.index  # So we can merge later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:34.266634Z","iopub.execute_input":"2025-08-01T08:06:34.266888Z","iopub.status.idle":"2025-08-01T08:06:34.693077Z","shell.execute_reply.started":"2025-08-01T08:06:34.266869Z","shell.execute_reply":"2025-08-01T08:06:34.692184Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Scale the Data\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train_tfidf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:34.694721Z","iopub.execute_input":"2025-08-01T08:06:34.694971Z","iopub.status.idle":"2025-08-01T08:06:36.156423Z","shell.execute_reply.started":"2025-08-01T08:06:34.694952Z","shell.execute_reply":"2025-08-01T08:06:36.155523Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Apply the PCA\npca = PCA(n_components=0.50)\ntrain_pca = pca.fit_transform(train_scaled)\n\ntrain_pca_df = pd.DataFrame(train_pca, columns=[f\"PCA_{i+1}\" for i in range(train_pca.shape[1])], index=train_df.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:06:36.157512Z","iopub.execute_input":"2025-08-01T08:06:36.157887Z","iopub.status.idle":"2025-08-01T08:09:32.432042Z","shell.execute_reply.started":"2025-08-01T08:06:36.157865Z","shell.execute_reply":"2025-08-01T08:09:32.430876Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_df_fnl = pd.concat([train_df, train_pca_df], axis = 1)\ntrain_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:09:32.433056Z","iopub.execute_input":"2025-08-01T08:09:32.433425Z","iopub.status.idle":"2025-08-01T08:09:32.499875Z","shell.execute_reply.started":"2025-08-01T08:09:32.433394Z","shell.execute_reply":"2025-08-01T08:09:32.498915Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi,   , north, mp,     , fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [work, got, injur, secur, concern, must, settl...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self imag take quiz -0.081771 -0.083898   \n1                                     drown spirit wash  0.045133 -0.020062   \n2     lunch crew made night night long day peac love... -0.102266 -0.077306   \n3     baltimor citi    north mp      fort mchenri tu... -0.113125 -0.128287   \n4                    concur longer spend child harm mmk -0.084877 -0.062786   \n...                                                 ...       ...       ...   \n6084     work got injur secur concern must settl tortur -0.093739 -0.062913   \n6085  wow alright sansa shook head blink rapidli new... -0.111144 -0.114540   \n6086                             well feel like im fire -0.089198 -0.071518   \n6087  destroy zimmerman fan club twitter obliter ren... -0.107102  0.015778   \n6088  psd special olap world pre recogn hazard wast ... -0.113474  0.256519   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_1177  PCA_1178  \\\n0    -0.056561 -0.088035 -0.051834  0.619031  ...  0.142783  0.481812   \n1    -0.037846 -0.072852 -0.006315 -0.035116  ...  1.602681 -0.135783   \n2    -0.066704 -0.000926 -0.075748 -0.045665  ... -0.049220  0.246913   \n3     0.240315 -0.110209 -0.137487 -0.025825  ...  8.335793 -4.285306   \n4    -0.039423 -0.022109 -0.066308 -0.052131  ... -1.808606 -0.976440   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.063302 -0.070698 -0.080632 -0.075403  ...  2.813912 -0.180907   \n6085 -0.038543 -0.065219 -0.063372 -0.074911  ... -1.471872  0.355770   \n6086 -0.053366  0.033320 -0.029724 -0.010348  ... -0.408594  0.188136   \n6087 -0.077112 -0.110659 -0.117066 -0.072565  ...  0.137081  0.112680   \n6088 -0.097250 -0.263557  0.235158 -0.083755  ...  0.572858 -2.620342   \n\n      PCA_1179  PCA_1180  PCA_1181  PCA_1182  PCA_1183  PCA_1184  PCA_1185  \\\n0    -0.215890 -0.066292 -1.136678 -0.831191 -0.172309 -0.685000 -0.729436   \n1     1.038651  0.838980 -0.052299 -0.343928  0.240409  0.174168 -0.916541   \n2    -1.593456 -0.722533  0.208101 -0.293919  0.462922 -0.457073 -0.458133   \n3    -2.890552  3.269782  5.830762  3.924183 -2.320754 -0.867953 -5.543731   \n4    -1.680733  1.124999  0.667636 -1.756042  2.074628 -3.507022  1.991084   \n...        ...       ...       ...       ...       ...       ...       ...   \n6084  3.235155  0.377263  0.141711  2.957237  0.468470 -1.581295 -0.186921   \n6085  1.922146 -1.504906  3.387658 -2.724790  2.388234 -1.185526 -2.411593   \n6086 -0.051951 -0.089511 -0.257227  0.030366 -0.155967 -0.018267 -0.507889   \n6087  0.012523  0.199074 -0.641680 -0.328984  0.062118 -0.840016  0.947162   \n6088 -4.207824  0.397961  1.562600  1.289377 -1.331954 -2.039499  1.822097   \n\n      PCA_1186  \n0     0.876607  \n1     0.044347  \n2    -0.173036  \n3     2.258131  \n4     0.194354  \n...        ...  \n6084 -3.779953  \n6085 -2.784889  \n6086  0.162730  \n6087 -2.301163  \n6088  1.648183  \n\n[6089 rows x 1190 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_1177</th>\n      <th>PCA_1178</th>\n      <th>PCA_1179</th>\n      <th>PCA_1180</th>\n      <th>PCA_1181</th>\n      <th>PCA_1182</th>\n      <th>PCA_1183</th>\n      <th>PCA_1184</th>\n      <th>PCA_1185</th>\n      <th>PCA_1186</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self imag, take, quiz]</td>\n      <td>feel delug low self imag take quiz</td>\n      <td>-0.081771</td>\n      <td>-0.083898</td>\n      <td>-0.056561</td>\n      <td>-0.088035</td>\n      <td>-0.051834</td>\n      <td>0.619031</td>\n      <td>...</td>\n      <td>0.142783</td>\n      <td>0.481812</td>\n      <td>-0.215890</td>\n      <td>-0.066292</td>\n      <td>-1.136678</td>\n      <td>-0.831191</td>\n      <td>-0.172309</td>\n      <td>-0.685000</td>\n      <td>-0.729436</td>\n      <td>0.876607</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>0.045133</td>\n      <td>-0.020062</td>\n      <td>-0.037846</td>\n      <td>-0.072852</td>\n      <td>-0.006315</td>\n      <td>-0.035116</td>\n      <td>...</td>\n      <td>1.602681</td>\n      <td>-0.135783</td>\n      <td>1.038651</td>\n      <td>0.838980</td>\n      <td>-0.052299</td>\n      <td>-0.343928</td>\n      <td>0.240409</td>\n      <td>0.174168</td>\n      <td>-0.916541</td>\n      <td>0.044347</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.102266</td>\n      <td>-0.077306</td>\n      <td>-0.066704</td>\n      <td>-0.000926</td>\n      <td>-0.075748</td>\n      <td>-0.045665</td>\n      <td>...</td>\n      <td>-0.049220</td>\n      <td>0.246913</td>\n      <td>-1.593456</td>\n      <td>-0.722533</td>\n      <td>0.208101</td>\n      <td>-0.293919</td>\n      <td>0.462922</td>\n      <td>-0.457073</td>\n      <td>-0.458133</td>\n      <td>-0.173036</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi,   , north, mp,     , fort, mc...</td>\n      <td>baltimor citi    north mp      fort mchenri tu...</td>\n      <td>-0.113125</td>\n      <td>-0.128287</td>\n      <td>0.240315</td>\n      <td>-0.110209</td>\n      <td>-0.137487</td>\n      <td>-0.025825</td>\n      <td>...</td>\n      <td>8.335793</td>\n      <td>-4.285306</td>\n      <td>-2.890552</td>\n      <td>3.269782</td>\n      <td>5.830762</td>\n      <td>3.924183</td>\n      <td>-2.320754</td>\n      <td>-0.867953</td>\n      <td>-5.543731</td>\n      <td>2.258131</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.084877</td>\n      <td>-0.062786</td>\n      <td>-0.039423</td>\n      <td>-0.022109</td>\n      <td>-0.066308</td>\n      <td>-0.052131</td>\n      <td>...</td>\n      <td>-1.808606</td>\n      <td>-0.976440</td>\n      <td>-1.680733</td>\n      <td>1.124999</td>\n      <td>0.667636</td>\n      <td>-1.756042</td>\n      <td>2.074628</td>\n      <td>-3.507022</td>\n      <td>1.991084</td>\n      <td>0.194354</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[work, got, injur, secur, concern, must, settl...</td>\n      <td>work got injur secur concern must settl tortur</td>\n      <td>-0.093739</td>\n      <td>-0.062913</td>\n      <td>-0.063302</td>\n      <td>-0.070698</td>\n      <td>-0.080632</td>\n      <td>-0.075403</td>\n      <td>...</td>\n      <td>2.813912</td>\n      <td>-0.180907</td>\n      <td>3.235155</td>\n      <td>0.377263</td>\n      <td>0.141711</td>\n      <td>2.957237</td>\n      <td>0.468470</td>\n      <td>-1.581295</td>\n      <td>-0.186921</td>\n      <td>-3.779953</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.111144</td>\n      <td>-0.114540</td>\n      <td>-0.038543</td>\n      <td>-0.065219</td>\n      <td>-0.063372</td>\n      <td>-0.074911</td>\n      <td>...</td>\n      <td>-1.471872</td>\n      <td>0.355770</td>\n      <td>1.922146</td>\n      <td>-1.504906</td>\n      <td>3.387658</td>\n      <td>-2.724790</td>\n      <td>2.388234</td>\n      <td>-1.185526</td>\n      <td>-2.411593</td>\n      <td>-2.784889</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.089198</td>\n      <td>-0.071518</td>\n      <td>-0.053366</td>\n      <td>0.033320</td>\n      <td>-0.029724</td>\n      <td>-0.010348</td>\n      <td>...</td>\n      <td>-0.408594</td>\n      <td>0.188136</td>\n      <td>-0.051951</td>\n      <td>-0.089511</td>\n      <td>-0.257227</td>\n      <td>0.030366</td>\n      <td>-0.155967</td>\n      <td>-0.018267</td>\n      <td>-0.507889</td>\n      <td>0.162730</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.107102</td>\n      <td>0.015778</td>\n      <td>-0.077112</td>\n      <td>-0.110659</td>\n      <td>-0.117066</td>\n      <td>-0.072565</td>\n      <td>...</td>\n      <td>0.137081</td>\n      <td>0.112680</td>\n      <td>0.012523</td>\n      <td>0.199074</td>\n      <td>-0.641680</td>\n      <td>-0.328984</td>\n      <td>0.062118</td>\n      <td>-0.840016</td>\n      <td>0.947162</td>\n      <td>-2.301163</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113474</td>\n      <td>0.256519</td>\n      <td>-0.097250</td>\n      <td>-0.263557</td>\n      <td>0.235158</td>\n      <td>-0.083755</td>\n      <td>...</td>\n      <td>0.572858</td>\n      <td>-2.620342</td>\n      <td>-4.207824</td>\n      <td>0.397961</td>\n      <td>1.562600</td>\n      <td>1.289377</td>\n      <td>-1.331954</td>\n      <td>-2.039499</td>\n      <td>1.822097</td>\n      <td>1.648183</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 1190 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Find the Top PCA Contributing Words","metadata":{}},{"cell_type":"code","source":"def keep_only_top_words(word_lst):\n    return [word for word in word_lst if word in top_words]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:09:32.501639Z","iopub.execute_input":"2025-08-01T08:09:32.501942Z","iopub.status.idle":"2025-08-01T08:09:32.506468Z","shell.execute_reply.started":"2025-08-01T08:09:32.501920Z","shell.execute_reply":"2025-08-01T08:09:32.505569Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:09:32.507489Z","iopub.execute_input":"2025-08-01T08:09:32.507859Z","iopub.status.idle":"2025-08-01T08:09:32.554972Z","shell.execute_reply.started":"2025-08-01T08:09:32.507837Z","shell.execute_reply":"2025-08-01T08:09:32.553692Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi,   , north, mp,     , fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [work, got, injur, secur, concern, must, settl...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self imag take quiz -0.081771 -0.083898   \n1                                     drown spirit wash  0.045133 -0.020062   \n2     lunch crew made night night long day peac love... -0.102266 -0.077306   \n3     baltimor citi    north mp      fort mchenri tu... -0.113125 -0.128287   \n4                    concur longer spend child harm mmk -0.084877 -0.062786   \n...                                                 ...       ...       ...   \n6084     work got injur secur concern must settl tortur -0.093739 -0.062913   \n6085  wow alright sansa shook head blink rapidli new... -0.111144 -0.114540   \n6086                             well feel like im fire -0.089198 -0.071518   \n6087  destroy zimmerman fan club twitter obliter ren... -0.107102  0.015778   \n6088  psd special olap world pre recogn hazard wast ... -0.113474  0.256519   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_1177  PCA_1178  \\\n0    -0.056561 -0.088035 -0.051834  0.619031  ...  0.142783  0.481812   \n1    -0.037846 -0.072852 -0.006315 -0.035116  ...  1.602681 -0.135783   \n2    -0.066704 -0.000926 -0.075748 -0.045665  ... -0.049220  0.246913   \n3     0.240315 -0.110209 -0.137487 -0.025825  ...  8.335793 -4.285306   \n4    -0.039423 -0.022109 -0.066308 -0.052131  ... -1.808606 -0.976440   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.063302 -0.070698 -0.080632 -0.075403  ...  2.813912 -0.180907   \n6085 -0.038543 -0.065219 -0.063372 -0.074911  ... -1.471872  0.355770   \n6086 -0.053366  0.033320 -0.029724 -0.010348  ... -0.408594  0.188136   \n6087 -0.077112 -0.110659 -0.117066 -0.072565  ...  0.137081  0.112680   \n6088 -0.097250 -0.263557  0.235158 -0.083755  ...  0.572858 -2.620342   \n\n      PCA_1179  PCA_1180  PCA_1181  PCA_1182  PCA_1183  PCA_1184  PCA_1185  \\\n0    -0.215890 -0.066292 -1.136678 -0.831191 -0.172309 -0.685000 -0.729436   \n1     1.038651  0.838980 -0.052299 -0.343928  0.240409  0.174168 -0.916541   \n2    -1.593456 -0.722533  0.208101 -0.293919  0.462922 -0.457073 -0.458133   \n3    -2.890552  3.269782  5.830762  3.924183 -2.320754 -0.867953 -5.543731   \n4    -1.680733  1.124999  0.667636 -1.756042  2.074628 -3.507022  1.991084   \n...        ...       ...       ...       ...       ...       ...       ...   \n6084  3.235155  0.377263  0.141711  2.957237  0.468470 -1.581295 -0.186921   \n6085  1.922146 -1.504906  3.387658 -2.724790  2.388234 -1.185526 -2.411593   \n6086 -0.051951 -0.089511 -0.257227  0.030366 -0.155967 -0.018267 -0.507889   \n6087  0.012523  0.199074 -0.641680 -0.328984  0.062118 -0.840016  0.947162   \n6088 -4.207824  0.397961  1.562600  1.289377 -1.331954 -2.039499  1.822097   \n\n      PCA_1186  \n0     0.876607  \n1     0.044347  \n2    -0.173036  \n3     2.258131  \n4     0.194354  \n...        ...  \n6084 -3.779953  \n6085 -2.784889  \n6086  0.162730  \n6087 -2.301163  \n6088  1.648183  \n\n[6089 rows x 1190 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_1177</th>\n      <th>PCA_1178</th>\n      <th>PCA_1179</th>\n      <th>PCA_1180</th>\n      <th>PCA_1181</th>\n      <th>PCA_1182</th>\n      <th>PCA_1183</th>\n      <th>PCA_1184</th>\n      <th>PCA_1185</th>\n      <th>PCA_1186</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self imag, take, quiz]</td>\n      <td>feel delug low self imag take quiz</td>\n      <td>-0.081771</td>\n      <td>-0.083898</td>\n      <td>-0.056561</td>\n      <td>-0.088035</td>\n      <td>-0.051834</td>\n      <td>0.619031</td>\n      <td>...</td>\n      <td>0.142783</td>\n      <td>0.481812</td>\n      <td>-0.215890</td>\n      <td>-0.066292</td>\n      <td>-1.136678</td>\n      <td>-0.831191</td>\n      <td>-0.172309</td>\n      <td>-0.685000</td>\n      <td>-0.729436</td>\n      <td>0.876607</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>0.045133</td>\n      <td>-0.020062</td>\n      <td>-0.037846</td>\n      <td>-0.072852</td>\n      <td>-0.006315</td>\n      <td>-0.035116</td>\n      <td>...</td>\n      <td>1.602681</td>\n      <td>-0.135783</td>\n      <td>1.038651</td>\n      <td>0.838980</td>\n      <td>-0.052299</td>\n      <td>-0.343928</td>\n      <td>0.240409</td>\n      <td>0.174168</td>\n      <td>-0.916541</td>\n      <td>0.044347</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.102266</td>\n      <td>-0.077306</td>\n      <td>-0.066704</td>\n      <td>-0.000926</td>\n      <td>-0.075748</td>\n      <td>-0.045665</td>\n      <td>...</td>\n      <td>-0.049220</td>\n      <td>0.246913</td>\n      <td>-1.593456</td>\n      <td>-0.722533</td>\n      <td>0.208101</td>\n      <td>-0.293919</td>\n      <td>0.462922</td>\n      <td>-0.457073</td>\n      <td>-0.458133</td>\n      <td>-0.173036</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi,   , north, mp,     , fort, mc...</td>\n      <td>baltimor citi    north mp      fort mchenri tu...</td>\n      <td>-0.113125</td>\n      <td>-0.128287</td>\n      <td>0.240315</td>\n      <td>-0.110209</td>\n      <td>-0.137487</td>\n      <td>-0.025825</td>\n      <td>...</td>\n      <td>8.335793</td>\n      <td>-4.285306</td>\n      <td>-2.890552</td>\n      <td>3.269782</td>\n      <td>5.830762</td>\n      <td>3.924183</td>\n      <td>-2.320754</td>\n      <td>-0.867953</td>\n      <td>-5.543731</td>\n      <td>2.258131</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.084877</td>\n      <td>-0.062786</td>\n      <td>-0.039423</td>\n      <td>-0.022109</td>\n      <td>-0.066308</td>\n      <td>-0.052131</td>\n      <td>...</td>\n      <td>-1.808606</td>\n      <td>-0.976440</td>\n      <td>-1.680733</td>\n      <td>1.124999</td>\n      <td>0.667636</td>\n      <td>-1.756042</td>\n      <td>2.074628</td>\n      <td>-3.507022</td>\n      <td>1.991084</td>\n      <td>0.194354</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[work, got, injur, secur, concern, must, settl...</td>\n      <td>work got injur secur concern must settl tortur</td>\n      <td>-0.093739</td>\n      <td>-0.062913</td>\n      <td>-0.063302</td>\n      <td>-0.070698</td>\n      <td>-0.080632</td>\n      <td>-0.075403</td>\n      <td>...</td>\n      <td>2.813912</td>\n      <td>-0.180907</td>\n      <td>3.235155</td>\n      <td>0.377263</td>\n      <td>0.141711</td>\n      <td>2.957237</td>\n      <td>0.468470</td>\n      <td>-1.581295</td>\n      <td>-0.186921</td>\n      <td>-3.779953</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.111144</td>\n      <td>-0.114540</td>\n      <td>-0.038543</td>\n      <td>-0.065219</td>\n      <td>-0.063372</td>\n      <td>-0.074911</td>\n      <td>...</td>\n      <td>-1.471872</td>\n      <td>0.355770</td>\n      <td>1.922146</td>\n      <td>-1.504906</td>\n      <td>3.387658</td>\n      <td>-2.724790</td>\n      <td>2.388234</td>\n      <td>-1.185526</td>\n      <td>-2.411593</td>\n      <td>-2.784889</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.089198</td>\n      <td>-0.071518</td>\n      <td>-0.053366</td>\n      <td>0.033320</td>\n      <td>-0.029724</td>\n      <td>-0.010348</td>\n      <td>...</td>\n      <td>-0.408594</td>\n      <td>0.188136</td>\n      <td>-0.051951</td>\n      <td>-0.089511</td>\n      <td>-0.257227</td>\n      <td>0.030366</td>\n      <td>-0.155967</td>\n      <td>-0.018267</td>\n      <td>-0.507889</td>\n      <td>0.162730</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.107102</td>\n      <td>0.015778</td>\n      <td>-0.077112</td>\n      <td>-0.110659</td>\n      <td>-0.117066</td>\n      <td>-0.072565</td>\n      <td>...</td>\n      <td>0.137081</td>\n      <td>0.112680</td>\n      <td>0.012523</td>\n      <td>0.199074</td>\n      <td>-0.641680</td>\n      <td>-0.328984</td>\n      <td>0.062118</td>\n      <td>-0.840016</td>\n      <td>0.947162</td>\n      <td>-2.301163</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113474</td>\n      <td>0.256519</td>\n      <td>-0.097250</td>\n      <td>-0.263557</td>\n      <td>0.235158</td>\n      <td>-0.083755</td>\n      <td>...</td>\n      <td>0.572858</td>\n      <td>-2.620342</td>\n      <td>-4.207824</td>\n      <td>0.397961</td>\n      <td>1.562600</td>\n      <td>1.289377</td>\n      <td>-1.331954</td>\n      <td>-2.039499</td>\n      <td>1.822097</td>\n      <td>1.648183</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 1190 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"feature_names = vectorizer.get_feature_names_out()\nn_feature_words = len(feature_names)\n\nimportance = np.sum(np.abs(pca.components_), axis=0)\n\ntop_indices = [i for i, val in enumerate(importance) if val > np.percentile(importance, 50)]\ntop_words = feature_names[top_indices]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:09:32.555951Z","iopub.execute_input":"2025-08-01T08:09:32.556218Z","iopub.status.idle":"2025-08-01T08:09:35.316059Z","shell.execute_reply.started":"2025-08-01T08:09:32.556199Z","shell.execute_reply":"2025-08-01T08:09:35.315143Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"n_feature_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:09:35.317385Z","iopub.execute_input":"2025-08-01T08:09:35.317804Z","iopub.status.idle":"2025-08-01T08:09:35.323700Z","shell.execute_reply.started":"2025-08-01T08:09:35.317780Z","shell.execute_reply":"2025-08-01T08:09:35.322244Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"9851"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"train_df_fnl[\"top_words\"] = train_df_fnl[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\ntrain_df_fnl[\"top_word_combine\"] = train_df_fnl[\"top_words\"].apply(lambda words: \" \".join(words))\nval_df_fnl = val_df.copy()\ntest_df_fnl = test_df.copy()\nval_df_fnl[\"top_words\"] = val_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\nval_df_fnl[\"top_word_combine\"] = val_df_fnl[\"top_words\"].apply(lambda words: \" \".join(words))\ntest_df_fnl[\"top_words\"]= test_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\ntest_df_fnl[\"top_word_combine\"] = test_df_fnl[\"top_words\"].apply(lambda words: \" \".join(words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:10:07.757288Z","iopub.execute_input":"2025-08-01T08:10:07.758302Z","iopub.status.idle":"2025-08-01T08:10:18.234899Z","shell.execute_reply.started":"2025-08-01T08:10:07.758270Z","shell.execute_reply":"2025-08-01T08:10:18.233945Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:10:18.236429Z","iopub.execute_input":"2025-08-01T08:10:18.236772Z","iopub.status.idle":"2025-08-01T08:10:18.272072Z","shell.execute_reply.started":"2025-08-01T08:10:18.236745Z","shell.execute_reply":"2025-08-01T08:10:18.270980Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi,   , north, mp,     , fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [work, got, injur, secur, concern, must, settl...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self imag take quiz -0.081771 -0.083898   \n1                                     drown spirit wash  0.045133 -0.020062   \n2     lunch crew made night night long day peac love... -0.102266 -0.077306   \n3     baltimor citi    north mp      fort mchenri tu... -0.113125 -0.128287   \n4                    concur longer spend child harm mmk -0.084877 -0.062786   \n...                                                 ...       ...       ...   \n6084     work got injur secur concern must settl tortur -0.093739 -0.062913   \n6085  wow alright sansa shook head blink rapidli new... -0.111144 -0.114540   \n6086                             well feel like im fire -0.089198 -0.071518   \n6087  destroy zimmerman fan club twitter obliter ren... -0.107102  0.015778   \n6088  psd special olap world pre recogn hazard wast ... -0.113474  0.256519   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_1179  PCA_1180  \\\n0    -0.056561 -0.088035 -0.051834  0.619031  ... -0.215890 -0.066292   \n1    -0.037846 -0.072852 -0.006315 -0.035116  ...  1.038651  0.838980   \n2    -0.066704 -0.000926 -0.075748 -0.045665  ... -1.593456 -0.722533   \n3     0.240315 -0.110209 -0.137487 -0.025825  ... -2.890552  3.269782   \n4    -0.039423 -0.022109 -0.066308 -0.052131  ... -1.680733  1.124999   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.063302 -0.070698 -0.080632 -0.075403  ...  3.235155  0.377263   \n6085 -0.038543 -0.065219 -0.063372 -0.074911  ...  1.922146 -1.504906   \n6086 -0.053366  0.033320 -0.029724 -0.010348  ... -0.051951 -0.089511   \n6087 -0.077112 -0.110659 -0.117066 -0.072565  ...  0.012523  0.199074   \n6088 -0.097250 -0.263557  0.235158 -0.083755  ... -4.207824  0.397961   \n\n      PCA_1181  PCA_1182  PCA_1183  PCA_1184  PCA_1185  PCA_1186  \\\n0    -1.136678 -0.831191 -0.172309 -0.685000 -0.729436  0.876607   \n1    -0.052299 -0.343928  0.240409  0.174168 -0.916541  0.044347   \n2     0.208101 -0.293919  0.462922 -0.457073 -0.458133 -0.173036   \n3     5.830762  3.924183 -2.320754 -0.867953 -5.543731  2.258131   \n4     0.667636 -1.756042  2.074628 -3.507022  1.991084  0.194354   \n...        ...       ...       ...       ...       ...       ...   \n6084  0.141711  2.957237  0.468470 -1.581295 -0.186921 -3.779953   \n6085  3.387658 -2.724790  2.388234 -1.185526 -2.411593 -2.784889   \n6086 -0.257227  0.030366 -0.155967 -0.018267 -0.507889  0.162730   \n6087 -0.641680 -0.328984  0.062118 -0.840016  0.947162 -2.301163   \n6088  1.562600  1.289377 -1.331954 -2.039499  1.822097  1.648183   \n\n                                              top_words  \\\n0                        [feel, delug, low, take, quiz]   \n1                                              [spirit]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [citi, north, fort, mchenri, tunnel, bore, col...   \n4                                        [spend, child]   \n...                                                 ...   \n6084                    [work, got, injur, secur, must]   \n6085              [shook, rapidli, new, inform, realli]   \n6086                           [well, feel, like, fire]   \n6087  [destroy, zimmerman, fan, twitter, renewsit, r...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                                       top_word_combine  \n0                              feel delug low take quiz  \n1                                                spirit  \n2       lunch crew made night night long day peac rescu  \n3     citi north fort mchenri tunnel bore collis nor...  \n4                                           spend child  \n...                                                 ...  \n6084                          work got injur secur must  \n6085                    shook rapidli new inform realli  \n6086                                well feel like fire  \n6087  destroy zimmerman fan twitter renewsit reduc s...  \n6088  psd special olap world pre recogn hazard wast ...  \n\n[6089 rows x 1192 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_1179</th>\n      <th>PCA_1180</th>\n      <th>PCA_1181</th>\n      <th>PCA_1182</th>\n      <th>PCA_1183</th>\n      <th>PCA_1184</th>\n      <th>PCA_1185</th>\n      <th>PCA_1186</th>\n      <th>top_words</th>\n      <th>top_word_combine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self imag, take, quiz]</td>\n      <td>feel delug low self imag take quiz</td>\n      <td>-0.081771</td>\n      <td>-0.083898</td>\n      <td>-0.056561</td>\n      <td>-0.088035</td>\n      <td>-0.051834</td>\n      <td>0.619031</td>\n      <td>...</td>\n      <td>-0.215890</td>\n      <td>-0.066292</td>\n      <td>-1.136678</td>\n      <td>-0.831191</td>\n      <td>-0.172309</td>\n      <td>-0.685000</td>\n      <td>-0.729436</td>\n      <td>0.876607</td>\n      <td>[feel, delug, low, take, quiz]</td>\n      <td>feel delug low take quiz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>0.045133</td>\n      <td>-0.020062</td>\n      <td>-0.037846</td>\n      <td>-0.072852</td>\n      <td>-0.006315</td>\n      <td>-0.035116</td>\n      <td>...</td>\n      <td>1.038651</td>\n      <td>0.838980</td>\n      <td>-0.052299</td>\n      <td>-0.343928</td>\n      <td>0.240409</td>\n      <td>0.174168</td>\n      <td>-0.916541</td>\n      <td>0.044347</td>\n      <td>[spirit]</td>\n      <td>spirit</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.102266</td>\n      <td>-0.077306</td>\n      <td>-0.066704</td>\n      <td>-0.000926</td>\n      <td>-0.075748</td>\n      <td>-0.045665</td>\n      <td>...</td>\n      <td>-1.593456</td>\n      <td>-0.722533</td>\n      <td>0.208101</td>\n      <td>-0.293919</td>\n      <td>0.462922</td>\n      <td>-0.457073</td>\n      <td>-0.458133</td>\n      <td>-0.173036</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac rescu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi,   , north, mp,     , fort, mc...</td>\n      <td>baltimor citi    north mp      fort mchenri tu...</td>\n      <td>-0.113125</td>\n      <td>-0.128287</td>\n      <td>0.240315</td>\n      <td>-0.110209</td>\n      <td>-0.137487</td>\n      <td>-0.025825</td>\n      <td>...</td>\n      <td>-2.890552</td>\n      <td>3.269782</td>\n      <td>5.830762</td>\n      <td>3.924183</td>\n      <td>-2.320754</td>\n      <td>-0.867953</td>\n      <td>-5.543731</td>\n      <td>2.258131</td>\n      <td>[citi, north, fort, mchenri, tunnel, bore, col...</td>\n      <td>citi north fort mchenri tunnel bore collis nor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.084877</td>\n      <td>-0.062786</td>\n      <td>-0.039423</td>\n      <td>-0.022109</td>\n      <td>-0.066308</td>\n      <td>-0.052131</td>\n      <td>...</td>\n      <td>-1.680733</td>\n      <td>1.124999</td>\n      <td>0.667636</td>\n      <td>-1.756042</td>\n      <td>2.074628</td>\n      <td>-3.507022</td>\n      <td>1.991084</td>\n      <td>0.194354</td>\n      <td>[spend, child]</td>\n      <td>spend child</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[work, got, injur, secur, concern, must, settl...</td>\n      <td>work got injur secur concern must settl tortur</td>\n      <td>-0.093739</td>\n      <td>-0.062913</td>\n      <td>-0.063302</td>\n      <td>-0.070698</td>\n      <td>-0.080632</td>\n      <td>-0.075403</td>\n      <td>...</td>\n      <td>3.235155</td>\n      <td>0.377263</td>\n      <td>0.141711</td>\n      <td>2.957237</td>\n      <td>0.468470</td>\n      <td>-1.581295</td>\n      <td>-0.186921</td>\n      <td>-3.779953</td>\n      <td>[work, got, injur, secur, must]</td>\n      <td>work got injur secur must</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.111144</td>\n      <td>-0.114540</td>\n      <td>-0.038543</td>\n      <td>-0.065219</td>\n      <td>-0.063372</td>\n      <td>-0.074911</td>\n      <td>...</td>\n      <td>1.922146</td>\n      <td>-1.504906</td>\n      <td>3.387658</td>\n      <td>-2.724790</td>\n      <td>2.388234</td>\n      <td>-1.185526</td>\n      <td>-2.411593</td>\n      <td>-2.784889</td>\n      <td>[shook, rapidli, new, inform, realli]</td>\n      <td>shook rapidli new inform realli</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.089198</td>\n      <td>-0.071518</td>\n      <td>-0.053366</td>\n      <td>0.033320</td>\n      <td>-0.029724</td>\n      <td>-0.010348</td>\n      <td>...</td>\n      <td>-0.051951</td>\n      <td>-0.089511</td>\n      <td>-0.257227</td>\n      <td>0.030366</td>\n      <td>-0.155967</td>\n      <td>-0.018267</td>\n      <td>-0.507889</td>\n      <td>0.162730</td>\n      <td>[well, feel, like, fire]</td>\n      <td>well feel like fire</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.107102</td>\n      <td>0.015778</td>\n      <td>-0.077112</td>\n      <td>-0.110659</td>\n      <td>-0.117066</td>\n      <td>-0.072565</td>\n      <td>...</td>\n      <td>0.012523</td>\n      <td>0.199074</td>\n      <td>-0.641680</td>\n      <td>-0.328984</td>\n      <td>0.062118</td>\n      <td>-0.840016</td>\n      <td>0.947162</td>\n      <td>-2.301163</td>\n      <td>[destroy, zimmerman, fan, twitter, renewsit, r...</td>\n      <td>destroy zimmerman fan twitter renewsit reduc s...</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113474</td>\n      <td>0.256519</td>\n      <td>-0.097250</td>\n      <td>-0.263557</td>\n      <td>0.235158</td>\n      <td>-0.083755</td>\n      <td>...</td>\n      <td>-4.207824</td>\n      <td>0.397961</td>\n      <td>1.562600</td>\n      <td>1.289377</td>\n      <td>-1.331954</td>\n      <td>-2.039499</td>\n      <td>1.822097</td>\n      <td>1.648183</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 1192 columns</p>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"len(top_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:10:18.273221Z","iopub.execute_input":"2025-08-01T08:10:18.273567Z","iopub.status.idle":"2025-08-01T08:10:18.280292Z","shell.execute_reply.started":"2025-08-01T08:10:18.273532Z","shell.execute_reply":"2025-08-01T08:10:18.279284Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"4925"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"# 3. Build Word Dictionary\nUse the training dataset to build the word dictionary. The word dictionary will use the ($word_i$, $label_i$) as key and the count of ($word_i$, $label_i$) occurrence as value","metadata":{}},{"cell_type":"code","source":"def build_word_dict(label_arr, token_word_arr):\n    \"\"\"Build frequencies.\n    Input:\n        token_word: a series of list of tokenized word\n        label: a series of label that match the array of the list of tokenized word\n    Output:\n        freqs: a dictionary mapping each (word, label) pair to its frequency\n    \"\"\"\n    word_dict = {}\n    y_list = list(label_arr) # make array into list\n\n    for label_idx in range(len(y_list)):\n\n        for word in token_word_arr[label_idx]:\n            word_dict[(word, y_list[label_idx])] = word_dict.get((word, y_list[label_idx]), 0) + 1\n    return word_dict\n        \nword_dict = build_word_dict(train_df_fnl['target'], train_df_fnl[\"top_words\"])    \n  \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:38.720631Z","iopub.execute_input":"2025-08-01T08:20:38.722009Z","iopub.status.idle":"2025-08-01T08:20:38.765173Z","shell.execute_reply.started":"2025-08-01T08:20:38.721960Z","shell.execute_reply":"2025-08-01T08:20:38.763647Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(\"number of words:\", len(word_dict))\nprint(\"Output Example:\", list(word_dict.items())[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:40.380623Z","iopub.execute_input":"2025-08-01T08:20:40.381562Z","iopub.status.idle":"2025-08-01T08:20:40.393804Z","shell.execute_reply.started":"2025-08-01T08:20:40.381507Z","shell.execute_reply":"2025-08-01T08:20:40.392301Z"}},"outputs":[{"name":"stdout","text":"number of words: 6427\nOutput Example: [(('feel', 0), 64), (('delug', 0), 43), (('low', 0), 15), (('take', 0), 52), (('quiz', 0), 13), (('spirit', 0), 7), (('lunch', 0), 4), (('crew', 0), 2), (('made', 0), 28), (('night', 0), 31)]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Extract Information","metadata":{}},{"cell_type":"markdown","source":"$$X_m = [x_0, x_1, x_2]\n    =[1, \\sum_{i=1}^{m} Freq(_i, 1), \\sum_{i=1}^{m}  Freq(w_i,0)]$$\n\n$x_0$: bias\n\n$x_1$: number of postive label for this word from the dictionary\n\n$x_2$: number of negative label for this word from the dictionary\n\n$m$: number of training tweets","metadata":{}},{"cell_type":"code","source":"def extract_info(word_dict,token_word_arr):\n    \"\"\"Extract Information from Each Tweets.\n    Input:\n        token_word_arr: a series of list of tokenized word\n        word_dic: the dictionary with (w, label) as key and frequency as value\n    Output:\n        1. a list of total frequency for each word in the tweet that associate with disaster from the dictionary\n        2. a list of total frequency for each word in the tweet that associate with not disaster from the dictionary\n    \"\"\"\n    cnt_dis_word = 0\n    cnt_non_dis_word = 0\n    dis_lst = []\n    non_dis_lst = []\n\n    # iterate all rows\n    for idx in range(len(token_word_arr)):\n        # iterate all words at the token_word_arr[idx]\n        for word in token_word_arr[idx]:\n            cnt_dis_word += word_dict.get((word, 1), 0)\n            cnt_non_dis_word += word_dict.get((word, 0), 0)\n\n        dis_lst.append(cnt_dis_word)\n        non_dis_lst.append(cnt_non_dis_word)\n    return dis_lst, non_dis_lst\n    \n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:42.711472Z","iopub.execute_input":"2025-08-01T08:20:42.712046Z","iopub.status.idle":"2025-08-01T08:20:42.721538Z","shell.execute_reply.started":"2025-08-01T08:20:42.712006Z","shell.execute_reply":"2025-08-01T08:20:42.719991Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Train Dataset\ntrain_df_fnl['disaster_score'], train_df_fnl['non_disaster_score']= extract_info(word_dict,train_df_fnl[\"top_words\"])\ntrain_df_fnl['bias'] = [1]*len(train_df_fnl) # add bias term\nX_train, y_train = train_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']], train_df_fnl['target']\n\n# Validation Dataset\nval_df_fnl['disaster_score'], val_df_fnl['non_disaster_score']= extract_info(word_dict,val_df_fnl['top_words'])\nval_df_fnl['bias'] = [1]*len(val_df_fnl) # add bias term\nX_val, y_val = val_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']], val_df_fnl['target']\n\n# Test Dataset\ntest_df_fnl['disaster_score'], test_df_fnl['non_disaster_score']= extract_info(word_dict,test_df_fnl['top_words'])\ntest_df_fnl['bias'] = [1]*len(test_df_fnl) # add bias term\nX_test = test_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:45.841175Z","iopub.execute_input":"2025-08-01T08:20:45.841536Z","iopub.status.idle":"2025-08-01T08:20:45.947447Z","shell.execute_reply.started":"2025-08-01T08:20:45.841512Z","shell.execute_reply":"2025-08-01T08:20:45.945966Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"print(X_train.shape) #(m, 3)\nprint(X_val.shape) #(m, 3)\n\n\ny_train = y_train.to_numpy().reshape(-1, 1)\ny_val = y_val.to_numpy().reshape(-1, 1)\nprint(y_train.shape) #(m,1)\nprint(y_val.shape) # (m, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:47.203378Z","iopub.execute_input":"2025-08-01T08:20:47.203765Z","iopub.status.idle":"2025-08-01T08:20:47.211659Z","shell.execute_reply.started":"2025-08-01T08:20:47.203742Z","shell.execute_reply":"2025-08-01T08:20:47.210199Z"}},"outputs":[{"name":"stdout","text":"(6089, 3)\n(1524, 3)\n(6089, 1)\n(1524, 1)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Overfitting\nTo avoid overfitting, I plan to implement the L2 Regression\n$$\\lambda*\\frac{1}{2}\\mathbf{\\theta}^T\\mathbf{\\theta}$$","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"* Sigmoid Function\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} $$\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N = x^Tz$$\n\n* Loss Function\n$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n    * By Taking Derivative of Log of Likelihood: $$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j $$\n* Loss Function with Ridge Regression\n$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)+\\frac{\\lambda}{2m}\\mathbf{\\theta}^T\\mathbf{\\theta}$$\n    * By Taking Derivative of Log of Likelihood: $$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} (\\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j + \\lambda \\theta_j )$$\n* Gradient Descent\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n\n$\\alpha$: learning rate\n\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"Calculate sigmoid.\n    Input: a digit\n    Output: the sigmoid of the digit\n    \"\"\"   \n    z = np.clip(z, -500, 500)  # Prevent overflow\n    return 1/(1+np.exp(-z))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:51.470354Z","iopub.execute_input":"2025-08-01T08:20:51.470802Z","iopub.status.idle":"2025-08-01T08:20:51.476725Z","shell.execute_reply.started":"2025-08-01T08:20:51.470778Z","shell.execute_reply":"2025-08-01T08:20:51.475297Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def accuracy(pred_label, label_y):\n    \"\"\"Calculate accuracy.\n    Input: predicted Y and actual Y value\n    Output: a list contains 0 or 1 if the predicted Y is the same as label_y\n    \"\"\"  \n    match_list = (pred_label == label_y).astype(int)\n    accuracy_percentage = sum(match_list)/ len(match_list)\n    return accuracy_percentage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:52.626966Z","iopub.execute_input":"2025-08-01T08:20:52.627347Z","iopub.status.idle":"2025-08-01T08:20:52.633204Z","shell.execute_reply.started":"2025-08-01T08:20:52.627322Z","shell.execute_reply":"2025-08-01T08:20:52.632158Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def gradDescent(alpha, theta, train_X, train_Y, val_X, val_Y,iteration, lam):\n    \"\"\"Calculate sigmoid.\n    Input: a digit\n    Output: the sigmoid of the digit\n    \"\"\"   \n    # initialize the train and validation loss list, also the theta\n    train_loss = []\n    val_loss = []\n    train_accuracy = []\n    val_accuracy = []\n    theta = np.zeros((3,1))\n\n    # optimal_iter = 0\n\n    # number of examples\n    train_m = len(train_Y)  \n    val_m = len(val_Y)\n    \n    \n    for itera in range(iteration):\n\n        # calculate the predicted y\n        z = np.dot(train_X,theta) #(m,1)\n\n        # put the predicted y into sigmoid\n        sig = sigmoid(z) #(m,1)\n\n        # Compute the gradient\n        gradient = (1/train_m) * np.dot(train_X.T, (sig - train_Y)) #(3,1)\n        gradient += (lam / train_m) * np.r_[[[0]], theta[1:]]  # exclude bias\n\n\n        # update theta\n        theta = theta - gradient * alpha #(3,1)\n\n\n        # calculate the training loss\n        train_z = np.dot(train_X,theta)        \n        train_sig = sigmoid(train_z)\n        train_sig = np.clip(train_sig, 1e-10, 1 - 1e-10)  # Prevent log(0)\n        train_loss_val = (-1/train_m)* (train_Y.T @ np.log(train_sig)+(1-train_Y).T@ np.log(1-train_sig))[0,0]+(lam / (2 * train_m)) * np.sum(theta[1:] ** 2)\n        train_loss.append(train_loss_val)\n\n        # predict the training label\n        pred_label_train = (train_sig >= 0.5).astype(int)\n        # calculate the training accuray\n        train_accuracy.append(accuracy_score(pred_label_train,train_Y))\n        \n        \n        # calculate the validation loss\n        val_z = np.dot(val_X,theta)\n        val_sig = sigmoid(val_z)\n        val_sig = np.clip(val_sig, 1e-10, 1 - 1e-10)  # Prevent log(0)\n        val_loss_val = (-1/val_m)* (val_Y.T @ np.log(val_sig)+(1-val_Y).T@ np.log(1-val_sig))[0,0]\n        val_loss.append(val_loss_val)   \n        \n        # predict the validation label\n        pred_label_val = (val_sig  >= 0.5).astype(int)\n        # calculate the training accuray\n        val_accuracy.append(accuracy_score(pred_label_val,val_Y))\n        \n\n\n        \n        # # update the theta optimal theta\n\n        # curr_train_val_loss = np.log(train_loss_val) + np.log(val_loss_val)\n        \n        # if curr_train_val_loss < min_train_val_loss:\n        #     theta_optimal = theta\n        #     min_train_val_loss = curr_train_val_loss\n        #     optimal_iter = itera\n\n    \n    return theta, train_loss, val_loss, train_accuracy, val_accuracy\n    \n    \n    \n    \n    \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:54.798747Z","iopub.execute_input":"2025-08-01T08:20:54.799049Z","iopub.status.idle":"2025-08-01T08:20:54.810524Z","shell.execute_reply.started":"2025-08-01T08:20:54.799029Z","shell.execute_reply":"2025-08-01T08:20:54.809609Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# y_axis[int(5500/500)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:57.461923Z","iopub.execute_input":"2025-08-01T08:20:57.462263Z","iopub.status.idle":"2025-08-01T08:20:57.467490Z","shell.execute_reply.started":"2025-08-01T08:20:57.462240Z","shell.execute_reply":"2025-08-01T08:20:57.465962Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"theta = np.zeros((3,1))\nalpha = 0.0001\niteration = 1000\nlam = 0.0001\ntheta, train_loss, val_loss, train_accuracy, val_accuracy = gradDescent(alpha,theta,X_train,y_train, X_val, y_val,iteration, lam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:20:57.837113Z","iopub.execute_input":"2025-08-01T08:20:57.837462Z","iopub.status.idle":"2025-08-01T08:21:00.863667Z","shell.execute_reply.started":"2025-08-01T08:20:57.837438Z","shell.execute_reply":"2025-08-01T08:21:00.862610Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(theta)\nprint(np.mean(train_accuracy))\nprint(np.mean(val_accuracy))\nprint(train_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:21:00.865154Z","iopub.execute_input":"2025-08-01T08:21:00.866505Z","iopub.status.idle":"2025-08-01T08:21:00.874653Z","shell.execute_reply.started":"2025-08-01T08:21:00.866479Z","shell.execute_reply":"2025-08-01T08:21:00.873515Z"}},"outputs":[{"name":"stdout","text":"[[ 6.08402788e-04]\n [-1.07658292e+01]\n [-1.26001344e+01]]\n0.5108356051896863\n0.5108372703412073\n[0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4302841189029397, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.5692231893578584, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42995565774347183, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.5667597306618493, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42979142716373786, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42995565774347183, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996]\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# iterations = range(1, iteration + 1)\n# plt.figure(figsize=(20, 5))\n# plt.plot(iterations, train_accuracy, label='Train Accuracy', color='blue')\n# plt.plot(iterations, val_accuracy, label='Validation Accuracy', color='orange')\n# plt.xlabel('Iteration')\n# plt.ylabel('Accuracy')\n# plt.title('Train vs Test Loss over Iterations')\n# plt.legend()\n# plt.grid(True)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:32:00.896500Z","iopub.execute_input":"2025-07-28T05:32:00.896858Z","iopub.status.idle":"2025-07-28T05:32:00.914247Z","shell.execute_reply.started":"2025-07-28T05:32:00.896838Z","shell.execute_reply":"2025-07-28T05:32:00.913470Z"}},"outputs":[],"execution_count":154},{"cell_type":"markdown","source":"# Naive Bayes","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\ntrain_X_nb = vectorizer.fit_transform(train_df_fnl[\"top_word_combine\"])\nval_X_nb = vectorizer.transform(val_df_fnl[\"top_word_combine\"])\n\nclf = MultinomialNB(alpha=1.0)\nclf.fit(train_X_nb, train_df_fnl[\"target\"])\n\ntrain_pred = clf.predict(train_X_nb)\ntrain_accuracy = accuracy_score(train_df_fnl['target'], train_pred)\n\nval_pred = clf.predict(val_X_nb)\nval_accuracy = accuracy_score(val_df_fnl['target'], val_pred)\n\nprint(train_accuracy)\nprint(val_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:26:09.247318Z","iopub.execute_input":"2025-08-01T08:26:09.249003Z","iopub.status.idle":"2025-08-01T08:26:09.330699Z","shell.execute_reply.started":"2025-08-01T08:26:09.248948Z","shell.execute_reply":"2025-08-01T08:26:09.329321Z"}},"outputs":[{"name":"stdout","text":"0.8467728691082279\n0.7703412073490814\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"Reference: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)","metadata":{}}]}