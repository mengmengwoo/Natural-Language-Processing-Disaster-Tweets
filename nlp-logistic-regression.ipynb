{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":918121,"sourceType":"datasetVersion","datasetId":494054}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport matplotlib.pyplot as plt\nimport random\nimport re\nimport string\nimport pickle\nfrom nltk.corpus import stopwords # module for stop words\nfrom nltk.stem import PorterStemmer # module for stemming\nfrom nltk.tokenize import TweetTokenizer # module for tokenizing strings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-28T17:52:19.180881Z","iopub.execute_input":"2025-07-28T17:52:19.181167Z","iopub.status.idle":"2025-07-28T17:52:24.028331Z","shell.execute_reply.started":"2025-07-28T17:52:19.181144Z","shell.execute_reply":"2025-07-28T17:52:24.027217Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"stopwords.words('english')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # download the stop words\n# nltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:14.659729Z","iopub.execute_input":"2025-07-28T05:06:14.660087Z","iopub.status.idle":"2025-07-28T05:06:14.666129Z","shell.execute_reply.started":"2025-07-28T05:06:14.660060Z","shell.execute_reply":"2025-07-28T05:06:14.665245Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 2. Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Explore the dataset\nLook at the number of training dataset that is with label 1 (related to disaster) versus lable 0 (not related to disaster)","metadata":{}},{"cell_type":"code","source":"# import train dataset\nraw_df = pd.read_csv(\"/kaggle/input/nlpgettingstarted/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlpgettingstarted/test.csv\")\n# check the number of dataset with label 1 and 0 \nprint(\"number of disaster sample:\", len(raw_df[raw_df[\"target\"] == 1]))\nprint(\"number of not disaster sample:\", len(raw_df[raw_df[\"target\"] == 0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:21.977824Z","iopub.execute_input":"2025-07-28T05:06:21.978271Z","iopub.status.idle":"2025-07-28T05:06:22.114057Z","shell.execute_reply.started":"2025-07-28T05:06:21.978247Z","shell.execute_reply":"2025-07-28T05:06:22.113295Z"}},"outputs":[{"name":"stdout","text":"number of disaster sample: 3271\nnumber of not disaster sample: 4342\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2.2 Train/Validation Split\nSince the test dataset is given, I am creating the train and validation sets to find the best $\\theta$ that minimize the cost without overfitting\n* Train: 80% of disaster sample + 80% of not disaster sample\n* Validation: 20% of disaster sample + 20% of not disaster sample","metadata":{}},{"cell_type":"code","source":"# disaster sample\nX_train_dis, X_val_dis, y_train_dis, y_val_dis = train_test_split(raw_df[raw_df[\"target\"] == 1]['text'], \n                                                                    raw_df[raw_df[\"target\"] == 1]['target'],\n                                                                    test_size=0.20, random_state=42, shuffle=True)\n\n# not disaster sample \nX_train_ndis, X_val_ndis, y_train_ndis, y_val_ndis = train_test_split(raw_df[raw_df[\"target\"] == 0]['text'], \n                                                                    raw_df[raw_df[\"target\"] == 0]['target'],\n                                                                    test_size=0.20, random_state=42, shuffle=True)\nX_train = pd.concat([X_train_dis, X_train_ndis], axis = 0).reset_index(drop = True).to_frame()\ny_train = pd.concat([y_train_dis, y_train_ndis], axis = 0).reset_index(drop = True).to_frame()\ntrain_df = pd.concat([X_train, y_train], axis = 1)\ntrain_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle dataset\n\n\nX_val = pd.concat([X_val_dis, X_val_ndis], axis = 0).reset_index(drop = True).to_frame()\ny_val = pd.concat([y_val_dis, y_val_ndis], axis = 0).reset_index(drop = True).to_frame()\nval_df = pd.concat([X_val, y_val], axis = 1)\nval_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:24.319222Z","iopub.execute_input":"2025-07-28T05:06:24.319554Z","iopub.status.idle":"2025-07-28T05:06:24.340752Z","shell.execute_reply.started":"2025-07-28T05:06:24.319531Z","shell.execute_reply":"2025-07-28T05:06:24.339752Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:25.976673Z","iopub.execute_input":"2025-07-28T05:06:25.976985Z","iopub.status.idle":"2025-07-28T05:06:25.983282Z","shell.execute_reply.started":"2025-07-28T05:06:25.976962Z","shell.execute_reply":"2025-07-28T05:06:25.982590Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(6089, 2)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## 2.2. Data Cleaning and Tokenizing\n1. Remove the hash tag\n2. Remove hyperlink\n3. Remove any word that start with @\n4. Tokenize the text\n5. Remove stop words\n6. Remove punctuation\n7. Stemming","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer, which will make the string to be list and lowercase all the words\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n\n# Initialize stemmer, which will be used to stem the word\nstemmer = PorterStemmer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:27.674352Z","iopub.execute_input":"2025-07-28T05:06:27.674690Z","iopub.status.idle":"2025-07-28T05:06:27.679605Z","shell.execute_reply.started":"2025-07-28T05:06:27.674667Z","shell.execute_reply":"2025-07-28T05:06:27.678728Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def clean_tokenize(text):\n    \"\"\"Process text function.\n    Input:\n        text: the text of the tweet\n    Output:\n        clean_token: a list of words containing the processed tweet\n\n    \"\"\"\n    # remove hashtags\n    # only removing the hash # sign from the word\n    text = re.sub(r'#', '', text)\n    \n    # remove hyperlink\n    text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)\n\n    # remove @\n    text = re.sub('@.*? ', '', text)\n\n    # remove ...\n    text = re.sub('\\.\\.\\.', '', text)\n\n    # remove \\x89\n    text = re.sub(r'\\x89', '', text)\n\n    # remove not readable\n\n    # Tokenize the text, which will also lowercase the word\n    text_token = tokenizer.tokenize(text)\n\n    # remove stop words, punctuation and stem the word\n    clean_token = []\n    for word in text_token:\n        if (word not in stopwords.words('english') and  # remove stopwords\n            word not in string.punctuation):   # remove punctuation\n            # stemming\n            clean_word = stemmer.stem(word)\n            clean_token.append(clean_word)\n    return clean_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:29.409230Z","iopub.execute_input":"2025-07-28T05:06:29.409573Z","iopub.status.idle":"2025-07-28T05:06:29.416752Z","shell.execute_reply.started":"2025-07-28T05:06:29.409551Z","shell.execute_reply":"2025-07-28T05:06:29.415738Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# clean training dataset\n\ntrain_df['Remove_Hash_Link_At'] = train_df['text'].map(clean_tokenize)\n\n# clean validation dataset\nval_df['Remove_Hash_Link_At'] = val_df['text'].map(clean_tokenize)\n\n# clean testing datast\ntest_df['Remove_Hash_Link_At'] = test_df['text'].map(clean_tokenize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:32.241361Z","iopub.execute_input":"2025-07-28T05:06:32.241721Z","iopub.status.idle":"2025-07-28T05:06:51.696409Z","shell.execute_reply.started":"2025-07-28T05:06:32.241698Z","shell.execute_reply":"2025-07-28T05:06:51.695333Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Vanishing Gradient Return\n1. Apply TfidfVectorizer\n\n    a. TfidfVectorizer is a tool from scikit-learn that transforms a list of text documents into a matrix of TF-IDF features.\n\n   \n    b. TF-IDF = Term Frequency â€“ Inverse Document Frequency\n\n2. Scale the Dataset\n\n    a. Upon the first logistic regression training, I had discovered my model face the vanishing gradient return, where the gradient vector is too small for the model to update the $\\theta$ vector. To avoid this issue, I had decided to scale the data prior to training my logistic regression model.\n\n3. Apply PCA\n\n","metadata":{}},{"cell_type":"code","source":"# Combine the text to fit into the TfidVectorizer\ntrain_df[\"Remove_Hash_Link_At_Combine\"] = train_df['Remove_Hash_Link_At'].apply(lambda tokens: ' '.join(tokens))\n# val_df[\"Remove_Hash_Link_At_Combine\"] = val_df['Remove_Hash_Link_At'].apply(lambda tokens: ' '.join(tokens))\n# test_df[\"Remove_Hash_Link_At_Combine\"] = test_df['Remove_Hash_Link_At'].apply(lambda tokens: ' '.join(tokens))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:51.697757Z","iopub.execute_input":"2025-07-28T05:06:51.698361Z","iopub.status.idle":"2025-07-28T05:06:51.707206Z","shell.execute_reply.started":"2025-07-28T05:06:51.698329Z","shell.execute_reply":"2025-07-28T05:06:51.706267Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Apply TfidVectorizer\nvectorizer = TfidfVectorizer()\ntrain_tfidf = vectorizer.fit_transform(train_df[\"Remove_Hash_Link_At_Combine\"])\ntrain_tfidf = pd.DataFrame(train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\ntrain_tfidf.index = train_df.index  # So we can merge later","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:51.708076Z","iopub.execute_input":"2025-07-28T05:06:51.708383Z","iopub.status.idle":"2025-07-28T05:06:52.104400Z","shell.execute_reply.started":"2025-07-28T05:06:51.708360Z","shell.execute_reply":"2025-07-28T05:06:52.103493Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Scale the Data\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train_tfidf)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:52.105797Z","iopub.execute_input":"2025-07-28T05:06:52.106045Z","iopub.status.idle":"2025-07-28T05:06:53.588015Z","shell.execute_reply.started":"2025-07-28T05:06:52.106019Z","shell.execute_reply":"2025-07-28T05:06:53.587092Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Apply the PCA\npca = PCA(n_components=0.90)\ntrain_pca = pca.fit_transform(train_scaled)\n\ntrain_pca_df = pd.DataFrame(train_pca, columns=[f\"PCA_{i+1}\" for i in range(train_pca.shape[1])], index=train_df.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:06:55.713487Z","iopub.execute_input":"2025-07-28T05:06:55.714313Z","iopub.status.idle":"2025-07-28T05:09:56.648060Z","shell.execute_reply.started":"2025-07-28T05:06:55.714285Z","shell.execute_reply":"2025-07-28T05:09:56.647129Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_df_fnl = pd.concat([train_df, train_pca_df], axis = 1)\ntrain_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:09:56.649531Z","iopub.execute_input":"2025-07-28T05:09:56.649796Z","iopub.status.idle":"2025-07-28T05:09:56.751294Z","shell.execute_reply.started":"2025-07-28T05:09:56.649777Z","shell.execute_reply":"2025-07-28T05:09:56.750318Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self-imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi, 95, north, mp, 54.8, fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [could, would, work, got, injur, secur, concer...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self-imag take quiz -0.103809 -0.004820   \n1                                     drown spirit wash -0.022870 -0.046965   \n2     lunch crew made night night long day peac love... -0.118431 -0.081678   \n3     baltimor citi 95 north mp 54.8 fort mchenri tu... -0.101302 -0.088488   \n4                    concur longer spend child harm mmk -0.100515 -0.066034   \n...                                                 ...       ...       ...   \n6084  could would work got injur secur concern must ... -0.122435 -0.076033   \n6085  wow alright sansa shook head blink rapidli new... -0.124342 -0.096810   \n6086                             well feel like im fire -0.105781 -0.044915   \n6087  destroy zimmerman fan club twitter obliter ren... -0.119875 -0.094695   \n6088  psd special olap world pre recogn hazard wast ... -0.113895 -0.086954   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_3300  PCA_3301  \\\n0    -0.069849 -0.055792 -0.012707  0.547366  ...  0.185203 -0.286780   \n1    -0.039346 -0.056824  0.015039 -0.026105  ... -0.225472  0.067332   \n2    -0.069096 -0.011810 -0.048462 -0.031301  ...  0.164629  0.843779   \n3    -0.108686 -0.139330 -0.067866 -0.022211  ... -0.343020 -0.236108   \n4    -0.051609 -0.019156 -0.034221 -0.026097  ...  0.143876  0.072140   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.066928 -0.093147  0.077987 -0.098983  ...  1.070804  1.715058   \n6085 -0.091506 -0.067413 -0.027625 -0.052506  ...  0.912135  0.463873   \n6086 -0.061630  0.030155 -0.021312 -0.007631  ... -0.023965 -0.155702   \n6087 -0.017337 -0.098059 -0.046858 -0.042815  ...  0.086552 -0.082424   \n6088  0.125022 -0.117756  0.259621 -0.051785  ...  0.388260  0.190988   \n\n      PCA_3302  PCA_3303  PCA_3304  PCA_3305  PCA_3306  PCA_3307  PCA_3308  \\\n0    -0.260483 -0.044931 -0.468706  0.047521 -0.411941  0.150299 -0.348786   \n1     0.801644  0.099572 -0.470581 -0.142529 -0.776593  1.499534 -0.503235   \n2     1.346242  0.124176 -0.863437 -0.902139 -0.041927 -0.027119  1.128388   \n3    -0.023124  0.854553 -0.655261 -0.039618  0.113906 -0.637715  0.313348   \n4    -0.588477 -0.156334  0.277074 -0.421111  0.040377  0.072370 -0.105157   \n...        ...       ...       ...       ...       ...       ...       ...   \n6084 -1.994109 -0.346302 -0.331956  0.344637 -0.475023  0.285512  0.856661   \n6085  0.690574  0.501143 -0.360378 -0.333349 -0.049440 -0.095271 -0.206000   \n6086 -0.032324 -0.098806 -0.329315 -0.263997 -0.514500  0.417821  0.474201   \n6087 -0.142150 -0.047275  0.214931  0.369555  0.079218 -0.024805  0.057221   \n6088 -0.167022  0.498989  0.964625 -0.341183 -0.329364 -1.301500 -1.127608   \n\n      PCA_3309  \n0    -0.128443  \n1     0.083692  \n2     1.424497  \n3     0.124221  \n4    -0.400602  \n...        ...  \n6084 -1.022422  \n6085  0.604812  \n6086  0.152877  \n6087 -0.210481  \n6088  0.540629  \n\n[6089 rows x 3313 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_3300</th>\n      <th>PCA_3301</th>\n      <th>PCA_3302</th>\n      <th>PCA_3303</th>\n      <th>PCA_3304</th>\n      <th>PCA_3305</th>\n      <th>PCA_3306</th>\n      <th>PCA_3307</th>\n      <th>PCA_3308</th>\n      <th>PCA_3309</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self-imag, take, quiz]</td>\n      <td>feel delug low self-imag take quiz</td>\n      <td>-0.103809</td>\n      <td>-0.004820</td>\n      <td>-0.069849</td>\n      <td>-0.055792</td>\n      <td>-0.012707</td>\n      <td>0.547366</td>\n      <td>...</td>\n      <td>0.185203</td>\n      <td>-0.286780</td>\n      <td>-0.260483</td>\n      <td>-0.044931</td>\n      <td>-0.468706</td>\n      <td>0.047521</td>\n      <td>-0.411941</td>\n      <td>0.150299</td>\n      <td>-0.348786</td>\n      <td>-0.128443</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>-0.022870</td>\n      <td>-0.046965</td>\n      <td>-0.039346</td>\n      <td>-0.056824</td>\n      <td>0.015039</td>\n      <td>-0.026105</td>\n      <td>...</td>\n      <td>-0.225472</td>\n      <td>0.067332</td>\n      <td>0.801644</td>\n      <td>0.099572</td>\n      <td>-0.470581</td>\n      <td>-0.142529</td>\n      <td>-0.776593</td>\n      <td>1.499534</td>\n      <td>-0.503235</td>\n      <td>0.083692</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.118431</td>\n      <td>-0.081678</td>\n      <td>-0.069096</td>\n      <td>-0.011810</td>\n      <td>-0.048462</td>\n      <td>-0.031301</td>\n      <td>...</td>\n      <td>0.164629</td>\n      <td>0.843779</td>\n      <td>1.346242</td>\n      <td>0.124176</td>\n      <td>-0.863437</td>\n      <td>-0.902139</td>\n      <td>-0.041927</td>\n      <td>-0.027119</td>\n      <td>1.128388</td>\n      <td>1.424497</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi, 95, north, mp, 54.8, fort, mc...</td>\n      <td>baltimor citi 95 north mp 54.8 fort mchenri tu...</td>\n      <td>-0.101302</td>\n      <td>-0.088488</td>\n      <td>-0.108686</td>\n      <td>-0.139330</td>\n      <td>-0.067866</td>\n      <td>-0.022211</td>\n      <td>...</td>\n      <td>-0.343020</td>\n      <td>-0.236108</td>\n      <td>-0.023124</td>\n      <td>0.854553</td>\n      <td>-0.655261</td>\n      <td>-0.039618</td>\n      <td>0.113906</td>\n      <td>-0.637715</td>\n      <td>0.313348</td>\n      <td>0.124221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.100515</td>\n      <td>-0.066034</td>\n      <td>-0.051609</td>\n      <td>-0.019156</td>\n      <td>-0.034221</td>\n      <td>-0.026097</td>\n      <td>...</td>\n      <td>0.143876</td>\n      <td>0.072140</td>\n      <td>-0.588477</td>\n      <td>-0.156334</td>\n      <td>0.277074</td>\n      <td>-0.421111</td>\n      <td>0.040377</td>\n      <td>0.072370</td>\n      <td>-0.105157</td>\n      <td>-0.400602</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[could, would, work, got, injur, secur, concer...</td>\n      <td>could would work got injur secur concern must ...</td>\n      <td>-0.122435</td>\n      <td>-0.076033</td>\n      <td>-0.066928</td>\n      <td>-0.093147</td>\n      <td>0.077987</td>\n      <td>-0.098983</td>\n      <td>...</td>\n      <td>1.070804</td>\n      <td>1.715058</td>\n      <td>-1.994109</td>\n      <td>-0.346302</td>\n      <td>-0.331956</td>\n      <td>0.344637</td>\n      <td>-0.475023</td>\n      <td>0.285512</td>\n      <td>0.856661</td>\n      <td>-1.022422</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.124342</td>\n      <td>-0.096810</td>\n      <td>-0.091506</td>\n      <td>-0.067413</td>\n      <td>-0.027625</td>\n      <td>-0.052506</td>\n      <td>...</td>\n      <td>0.912135</td>\n      <td>0.463873</td>\n      <td>0.690574</td>\n      <td>0.501143</td>\n      <td>-0.360378</td>\n      <td>-0.333349</td>\n      <td>-0.049440</td>\n      <td>-0.095271</td>\n      <td>-0.206000</td>\n      <td>0.604812</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.105781</td>\n      <td>-0.044915</td>\n      <td>-0.061630</td>\n      <td>0.030155</td>\n      <td>-0.021312</td>\n      <td>-0.007631</td>\n      <td>...</td>\n      <td>-0.023965</td>\n      <td>-0.155702</td>\n      <td>-0.032324</td>\n      <td>-0.098806</td>\n      <td>-0.329315</td>\n      <td>-0.263997</td>\n      <td>-0.514500</td>\n      <td>0.417821</td>\n      <td>0.474201</td>\n      <td>0.152877</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.119875</td>\n      <td>-0.094695</td>\n      <td>-0.017337</td>\n      <td>-0.098059</td>\n      <td>-0.046858</td>\n      <td>-0.042815</td>\n      <td>...</td>\n      <td>0.086552</td>\n      <td>-0.082424</td>\n      <td>-0.142150</td>\n      <td>-0.047275</td>\n      <td>0.214931</td>\n      <td>0.369555</td>\n      <td>0.079218</td>\n      <td>-0.024805</td>\n      <td>0.057221</td>\n      <td>-0.210481</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113895</td>\n      <td>-0.086954</td>\n      <td>0.125022</td>\n      <td>-0.117756</td>\n      <td>0.259621</td>\n      <td>-0.051785</td>\n      <td>...</td>\n      <td>0.388260</td>\n      <td>0.190988</td>\n      <td>-0.167022</td>\n      <td>0.498989</td>\n      <td>0.964625</td>\n      <td>-0.341183</td>\n      <td>-0.329364</td>\n      <td>-1.301500</td>\n      <td>-1.127608</td>\n      <td>0.540629</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 3313 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Find the Top PCA Contributing Words","metadata":{}},{"cell_type":"code","source":"def keep_only_top_words(word_lst):\n    return [word for word in word_lst if word in top_words]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:09:56.752201Z","iopub.execute_input":"2025-07-28T05:09:56.752535Z","iopub.status.idle":"2025-07-28T05:09:56.757004Z","shell.execute_reply.started":"2025-07-28T05:09:56.752514Z","shell.execute_reply":"2025-07-28T05:09:56.756044Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:09:56.758632Z","iopub.execute_input":"2025-07-28T05:09:56.758867Z","iopub.status.idle":"2025-07-28T05:09:56.801738Z","shell.execute_reply.started":"2025-07-28T05:09:56.758849Z","shell.execute_reply":"2025-07-28T05:09:56.800878Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self-imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi, 95, north, mp, 54.8, fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [could, would, work, got, injur, secur, concer...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self-imag take quiz -0.103809 -0.004820   \n1                                     drown spirit wash -0.022870 -0.046965   \n2     lunch crew made night night long day peac love... -0.118431 -0.081678   \n3     baltimor citi 95 north mp 54.8 fort mchenri tu... -0.101302 -0.088488   \n4                    concur longer spend child harm mmk -0.100515 -0.066034   \n...                                                 ...       ...       ...   \n6084  could would work got injur secur concern must ... -0.122435 -0.076033   \n6085  wow alright sansa shook head blink rapidli new... -0.124342 -0.096810   \n6086                             well feel like im fire -0.105781 -0.044915   \n6087  destroy zimmerman fan club twitter obliter ren... -0.119875 -0.094695   \n6088  psd special olap world pre recogn hazard wast ... -0.113895 -0.086954   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_3300  PCA_3301  \\\n0    -0.069849 -0.055792 -0.012707  0.547366  ...  0.185203 -0.286780   \n1    -0.039346 -0.056824  0.015039 -0.026105  ... -0.225472  0.067332   \n2    -0.069096 -0.011810 -0.048462 -0.031301  ...  0.164629  0.843779   \n3    -0.108686 -0.139330 -0.067866 -0.022211  ... -0.343020 -0.236108   \n4    -0.051609 -0.019156 -0.034221 -0.026097  ...  0.143876  0.072140   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.066928 -0.093147  0.077987 -0.098983  ...  1.070804  1.715058   \n6085 -0.091506 -0.067413 -0.027625 -0.052506  ...  0.912135  0.463873   \n6086 -0.061630  0.030155 -0.021312 -0.007631  ... -0.023965 -0.155702   \n6087 -0.017337 -0.098059 -0.046858 -0.042815  ...  0.086552 -0.082424   \n6088  0.125022 -0.117756  0.259621 -0.051785  ...  0.388260  0.190988   \n\n      PCA_3302  PCA_3303  PCA_3304  PCA_3305  PCA_3306  PCA_3307  PCA_3308  \\\n0    -0.260483 -0.044931 -0.468706  0.047521 -0.411941  0.150299 -0.348786   \n1     0.801644  0.099572 -0.470581 -0.142529 -0.776593  1.499534 -0.503235   \n2     1.346242  0.124176 -0.863437 -0.902139 -0.041927 -0.027119  1.128388   \n3    -0.023124  0.854553 -0.655261 -0.039618  0.113906 -0.637715  0.313348   \n4    -0.588477 -0.156334  0.277074 -0.421111  0.040377  0.072370 -0.105157   \n...        ...       ...       ...       ...       ...       ...       ...   \n6084 -1.994109 -0.346302 -0.331956  0.344637 -0.475023  0.285512  0.856661   \n6085  0.690574  0.501143 -0.360378 -0.333349 -0.049440 -0.095271 -0.206000   \n6086 -0.032324 -0.098806 -0.329315 -0.263997 -0.514500  0.417821  0.474201   \n6087 -0.142150 -0.047275  0.214931  0.369555  0.079218 -0.024805  0.057221   \n6088 -0.167022  0.498989  0.964625 -0.341183 -0.329364 -1.301500 -1.127608   \n\n      PCA_3309  \n0    -0.128443  \n1     0.083692  \n2     1.424497  \n3     0.124221  \n4    -0.400602  \n...        ...  \n6084 -1.022422  \n6085  0.604812  \n6086  0.152877  \n6087 -0.210481  \n6088  0.540629  \n\n[6089 rows x 3313 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_3300</th>\n      <th>PCA_3301</th>\n      <th>PCA_3302</th>\n      <th>PCA_3303</th>\n      <th>PCA_3304</th>\n      <th>PCA_3305</th>\n      <th>PCA_3306</th>\n      <th>PCA_3307</th>\n      <th>PCA_3308</th>\n      <th>PCA_3309</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self-imag, take, quiz]</td>\n      <td>feel delug low self-imag take quiz</td>\n      <td>-0.103809</td>\n      <td>-0.004820</td>\n      <td>-0.069849</td>\n      <td>-0.055792</td>\n      <td>-0.012707</td>\n      <td>0.547366</td>\n      <td>...</td>\n      <td>0.185203</td>\n      <td>-0.286780</td>\n      <td>-0.260483</td>\n      <td>-0.044931</td>\n      <td>-0.468706</td>\n      <td>0.047521</td>\n      <td>-0.411941</td>\n      <td>0.150299</td>\n      <td>-0.348786</td>\n      <td>-0.128443</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>-0.022870</td>\n      <td>-0.046965</td>\n      <td>-0.039346</td>\n      <td>-0.056824</td>\n      <td>0.015039</td>\n      <td>-0.026105</td>\n      <td>...</td>\n      <td>-0.225472</td>\n      <td>0.067332</td>\n      <td>0.801644</td>\n      <td>0.099572</td>\n      <td>-0.470581</td>\n      <td>-0.142529</td>\n      <td>-0.776593</td>\n      <td>1.499534</td>\n      <td>-0.503235</td>\n      <td>0.083692</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.118431</td>\n      <td>-0.081678</td>\n      <td>-0.069096</td>\n      <td>-0.011810</td>\n      <td>-0.048462</td>\n      <td>-0.031301</td>\n      <td>...</td>\n      <td>0.164629</td>\n      <td>0.843779</td>\n      <td>1.346242</td>\n      <td>0.124176</td>\n      <td>-0.863437</td>\n      <td>-0.902139</td>\n      <td>-0.041927</td>\n      <td>-0.027119</td>\n      <td>1.128388</td>\n      <td>1.424497</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi, 95, north, mp, 54.8, fort, mc...</td>\n      <td>baltimor citi 95 north mp 54.8 fort mchenri tu...</td>\n      <td>-0.101302</td>\n      <td>-0.088488</td>\n      <td>-0.108686</td>\n      <td>-0.139330</td>\n      <td>-0.067866</td>\n      <td>-0.022211</td>\n      <td>...</td>\n      <td>-0.343020</td>\n      <td>-0.236108</td>\n      <td>-0.023124</td>\n      <td>0.854553</td>\n      <td>-0.655261</td>\n      <td>-0.039618</td>\n      <td>0.113906</td>\n      <td>-0.637715</td>\n      <td>0.313348</td>\n      <td>0.124221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.100515</td>\n      <td>-0.066034</td>\n      <td>-0.051609</td>\n      <td>-0.019156</td>\n      <td>-0.034221</td>\n      <td>-0.026097</td>\n      <td>...</td>\n      <td>0.143876</td>\n      <td>0.072140</td>\n      <td>-0.588477</td>\n      <td>-0.156334</td>\n      <td>0.277074</td>\n      <td>-0.421111</td>\n      <td>0.040377</td>\n      <td>0.072370</td>\n      <td>-0.105157</td>\n      <td>-0.400602</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[could, would, work, got, injur, secur, concer...</td>\n      <td>could would work got injur secur concern must ...</td>\n      <td>-0.122435</td>\n      <td>-0.076033</td>\n      <td>-0.066928</td>\n      <td>-0.093147</td>\n      <td>0.077987</td>\n      <td>-0.098983</td>\n      <td>...</td>\n      <td>1.070804</td>\n      <td>1.715058</td>\n      <td>-1.994109</td>\n      <td>-0.346302</td>\n      <td>-0.331956</td>\n      <td>0.344637</td>\n      <td>-0.475023</td>\n      <td>0.285512</td>\n      <td>0.856661</td>\n      <td>-1.022422</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.124342</td>\n      <td>-0.096810</td>\n      <td>-0.091506</td>\n      <td>-0.067413</td>\n      <td>-0.027625</td>\n      <td>-0.052506</td>\n      <td>...</td>\n      <td>0.912135</td>\n      <td>0.463873</td>\n      <td>0.690574</td>\n      <td>0.501143</td>\n      <td>-0.360378</td>\n      <td>-0.333349</td>\n      <td>-0.049440</td>\n      <td>-0.095271</td>\n      <td>-0.206000</td>\n      <td>0.604812</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.105781</td>\n      <td>-0.044915</td>\n      <td>-0.061630</td>\n      <td>0.030155</td>\n      <td>-0.021312</td>\n      <td>-0.007631</td>\n      <td>...</td>\n      <td>-0.023965</td>\n      <td>-0.155702</td>\n      <td>-0.032324</td>\n      <td>-0.098806</td>\n      <td>-0.329315</td>\n      <td>-0.263997</td>\n      <td>-0.514500</td>\n      <td>0.417821</td>\n      <td>0.474201</td>\n      <td>0.152877</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.119875</td>\n      <td>-0.094695</td>\n      <td>-0.017337</td>\n      <td>-0.098059</td>\n      <td>-0.046858</td>\n      <td>-0.042815</td>\n      <td>...</td>\n      <td>0.086552</td>\n      <td>-0.082424</td>\n      <td>-0.142150</td>\n      <td>-0.047275</td>\n      <td>0.214931</td>\n      <td>0.369555</td>\n      <td>0.079218</td>\n      <td>-0.024805</td>\n      <td>0.057221</td>\n      <td>-0.210481</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113895</td>\n      <td>-0.086954</td>\n      <td>0.125022</td>\n      <td>-0.117756</td>\n      <td>0.259621</td>\n      <td>-0.051785</td>\n      <td>...</td>\n      <td>0.388260</td>\n      <td>0.190988</td>\n      <td>-0.167022</td>\n      <td>0.498989</td>\n      <td>0.964625</td>\n      <td>-0.341183</td>\n      <td>-0.329364</td>\n      <td>-1.301500</td>\n      <td>-1.127608</td>\n      <td>0.540629</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 3313 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"feature_names = vectorizer.get_feature_names_out()\nn_feature_words = len(feature_names)\nX_axis = np.arange(500,10000+1,500)\ny_axis = []\n# Sum absolute weights across all components\nimportance = np.sum(np.abs(pca.components_), axis=0)\n\ntop_indices = [i for i, val in enumerate(importance) if val > np.percentile(importance, 50)]\ntop_words = feature_names[top_indices]\n\n# for top_k_words in np.arange(500,10000+1, 500):\n#     print(top_k_words)\n\n#     # Get top indices\n#     top_indices = importance.argsort()[-top_k_words:][::-1]\n#     top_words = feature_names[top_indices]     \n#     temp_df[\"top_words\"] = temp_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\n#     y_axis.append(len(temp_df[(temp_df[\"top_words\"].apply(len) == 0) & (temp_df[\"target\"] == 1)]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:43.388092Z","iopub.execute_input":"2025-07-28T05:31:43.388399Z","iopub.status.idle":"2025-07-28T05:31:46.432157Z","shell.execute_reply.started":"2025-07-28T05:31:43.388377Z","shell.execute_reply":"2025-07-28T05:31:46.431259Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"# feature_names = vectorizer.get_feature_names_out()\n# n_feature_words = len(feature_names)\n# X_axis = np.arange(500,10000+1,500)\n# y_axis = []\n# # Sum absolute weights across all components\n# importance = np.sum(np.abs(pca.components_), axis=0)\n\n# temp_df = train_df_fnl[[\"target\", \"Remove_Hash_Link_At\"]]\n# for top_k_words in np.arange(500,10000+1, 500):\n#     print(top_k_words)\n\n#     # Get top indices\n#     top_indices = importance.argsort()[-top_k_words:][::-1]\n#     top_words = feature_names[top_indices]     \n#     temp_df[\"top_words\"] = temp_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\n#     y_axis.append(len(temp_df[(temp_df[\"top_words\"].apply(len) == 0) & (temp_df[\"target\"] == 1)]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:46.433701Z","iopub.execute_input":"2025-07-28T05:31:46.434023Z","iopub.status.idle":"2025-07-28T05:31:46.438442Z","shell.execute_reply.started":"2025-07-28T05:31:46.433994Z","shell.execute_reply":"2025-07-28T05:31:46.437707Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"n_feature_words","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:46.439248Z","iopub.execute_input":"2025-07-28T05:31:46.439542Z","iopub.status.idle":"2025-07-28T05:31:46.456489Z","shell.execute_reply.started":"2025-07-28T05:31:46.439518Z","shell.execute_reply":"2025-07-28T05:31:46.455590Z"}},"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"10618"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"# plt.figure(figsize=(12, 6))\n# plt.plot(X_axis, y_axis, color='blue')\n# plt.title(\"Number of Empty Disaster Key Words vs. Number of Key Words\")\n# plt.xlabel(\"Number of Key Words\")\n# plt.ylabel(\"Number of Empty Disaster Key Words\")\n# plt.grid(True)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:46.458171Z","iopub.execute_input":"2025-07-28T05:31:46.458514Z","iopub.status.idle":"2025-07-28T05:31:46.472138Z","shell.execute_reply.started":"2025-07-28T05:31:46.458484Z","shell.execute_reply":"2025-07-28T05:31:46.471377Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"# top_k_words = 6000\n# top_indices = importance.argsort()[-top_k_words:][::-1]\n# top_words = feature_names[top_indices]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:46.473012Z","iopub.execute_input":"2025-07-28T05:31:46.473273Z","iopub.status.idle":"2025-07-28T05:31:46.488210Z","shell.execute_reply.started":"2025-07-28T05:31:46.473252Z","shell.execute_reply":"2025-07-28T05:31:46.487479Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"train_df_fnl[\"top_words\"] = train_df_fnl[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\nval_df_fnl = val_df.copy()\ntest_df_fnl = test_df.copy()\nval_df_fnl[\"top_words\"] = val_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)\ntest_df_fnl[\"top_words\"]= test_df[\"Remove_Hash_Link_At\"].apply(keep_only_top_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:46.489214Z","iopub.execute_input":"2025-07-28T05:31:46.489533Z","iopub.status.idle":"2025-07-28T05:31:57.403808Z","shell.execute_reply.started":"2025-07-28T05:31:46.489505Z","shell.execute_reply":"2025-07-28T05:31:57.402763Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"train_df_fnl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.404770Z","iopub.execute_input":"2025-07-28T05:31:57.405013Z","iopub.status.idle":"2025-07-28T05:31:57.439255Z","shell.execute_reply.started":"2025-07-28T05:31:57.404994Z","shell.execute_reply":"2025-07-28T05:31:57.438384Z"}},"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"                                                   text  target  \\\n0     Do you feel deluged by low self-image? Take th...       0   \n1               I'm drowning in spirits to wash you out       0   \n2     Lunch for the crew is made. Night night it's b...       0   \n3     Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...       1   \n4     I concur. The longer you spend with your child...       0   \n...                                                 ...     ...   \n6084  If I could I would have been by at work but go...       0   \n6085  @UntamedDirewolf 'I... Wow. Alright.' Sansa sh...       0   \n6086                     well it feels like im on fire.       0   \n6087  We destroyed the #Zimmerman fan club on Twitte...       0   \n6088  #psd #special Olap #world pres: http://t.co/9x...       1   \n\n                                    Remove_Hash_Link_At  \\\n0             [feel, delug, low, self-imag, take, quiz]   \n1                                 [drown, spirit, wash]   \n2     [lunch, crew, made, night, night, long, day, p...   \n3     [baltimor, citi, 95, north, mp, 54.8, fort, mc...   \n4             [concur, longer, spend, child, harm, mmk]   \n...                                                 ...   \n6084  [could, would, work, got, injur, secur, concer...   \n6085  [wow, alright, sansa, shook, head, blink, rapi...   \n6086                       [well, feel, like, im, fire]   \n6087  [destroy, zimmerman, fan, club, twitter, oblit...   \n6088  [psd, special, olap, world, pre, recogn, hazar...   \n\n                            Remove_Hash_Link_At_Combine     PCA_1     PCA_2  \\\n0                    feel delug low self-imag take quiz -0.103809 -0.004820   \n1                                     drown spirit wash -0.022870 -0.046965   \n2     lunch crew made night night long day peac love... -0.118431 -0.081678   \n3     baltimor citi 95 north mp 54.8 fort mchenri tu... -0.101302 -0.088488   \n4                    concur longer spend child harm mmk -0.100515 -0.066034   \n...                                                 ...       ...       ...   \n6084  could would work got injur secur concern must ... -0.122435 -0.076033   \n6085  wow alright sansa shook head blink rapidli new... -0.124342 -0.096810   \n6086                             well feel like im fire -0.105781 -0.044915   \n6087  destroy zimmerman fan club twitter obliter ren... -0.119875 -0.094695   \n6088  psd special olap world pre recogn hazard wast ... -0.113895 -0.086954   \n\n         PCA_3     PCA_4     PCA_5     PCA_6  ...  PCA_3304  PCA_3305  \\\n0    -0.069849 -0.055792 -0.012707  0.547366  ... -0.468706  0.047521   \n1    -0.039346 -0.056824  0.015039 -0.026105  ... -0.470581 -0.142529   \n2    -0.069096 -0.011810 -0.048462 -0.031301  ... -0.863437 -0.902139   \n3    -0.108686 -0.139330 -0.067866 -0.022211  ... -0.655261 -0.039618   \n4    -0.051609 -0.019156 -0.034221 -0.026097  ...  0.277074 -0.421111   \n...        ...       ...       ...       ...  ...       ...       ...   \n6084 -0.066928 -0.093147  0.077987 -0.098983  ... -0.331956  0.344637   \n6085 -0.091506 -0.067413 -0.027625 -0.052506  ... -0.360378 -0.333349   \n6086 -0.061630  0.030155 -0.021312 -0.007631  ... -0.329315 -0.263997   \n6087 -0.017337 -0.098059 -0.046858 -0.042815  ...  0.214931  0.369555   \n6088  0.125022 -0.117756  0.259621 -0.051785  ...  0.964625 -0.341183   \n\n      PCA_3306  PCA_3307  PCA_3308  PCA_3309  \\\n0    -0.411941  0.150299 -0.348786 -0.128443   \n1    -0.776593  1.499534 -0.503235  0.083692   \n2    -0.041927 -0.027119  1.128388  1.424497   \n3     0.113906 -0.637715  0.313348  0.124221   \n4     0.040377  0.072370 -0.105157 -0.400602   \n...        ...       ...       ...       ...   \n6084 -0.475023  0.285512  0.856661 -1.022422   \n6085 -0.049440 -0.095271 -0.206000  0.604812   \n6086 -0.514500  0.417821  0.474201  0.152877   \n6087  0.079218 -0.024805  0.057221 -0.210481   \n6088 -0.329364 -1.301500 -1.127608  0.540629   \n\n                                              top_words  disaster_score  \\\n0                        [feel, delug, low, take, quiz]              53   \n1                                               [drown]              78   \n2     [crew, made, night, night, long, day, peac, lo...             215   \n3     [citi, 95, north, fort, mchenri, bore, 95, nor...             294   \n4                     [concur, spend, child, harm, mmk]             308   \n...                                                 ...             ...   \n6084  [could, would, work, got, injur, secur, concer...          672435   \n6085  [wow, alright, sansa, head, blink, rapidli, ne...          672544   \n6086                       [well, feel, like, im, fire]          672874   \n6087      [destroy, fan, club, twitter, obliter, reduc]          672932   \n6088            [psd, world, pre, hazard, multidimensi]          672973   \n\n      non_disaster_score  bias  \n0                    144     1  \n1                    206     1  \n2                    535     1  \n3                    582     1  \n4                    627     1  \n...                  ...   ...  \n6084              874123     1  \n6085              874436     1  \n6086              874877     1  \n6087              875021     1  \n6088              875073     1  \n\n[6089 rows x 3317 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>Remove_Hash_Link_At</th>\n      <th>Remove_Hash_Link_At_Combine</th>\n      <th>PCA_1</th>\n      <th>PCA_2</th>\n      <th>PCA_3</th>\n      <th>PCA_4</th>\n      <th>PCA_5</th>\n      <th>PCA_6</th>\n      <th>...</th>\n      <th>PCA_3304</th>\n      <th>PCA_3305</th>\n      <th>PCA_3306</th>\n      <th>PCA_3307</th>\n      <th>PCA_3308</th>\n      <th>PCA_3309</th>\n      <th>top_words</th>\n      <th>disaster_score</th>\n      <th>non_disaster_score</th>\n      <th>bias</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you feel deluged by low self-image? Take th...</td>\n      <td>0</td>\n      <td>[feel, delug, low, self-imag, take, quiz]</td>\n      <td>feel delug low self-imag take quiz</td>\n      <td>-0.103809</td>\n      <td>-0.004820</td>\n      <td>-0.069849</td>\n      <td>-0.055792</td>\n      <td>-0.012707</td>\n      <td>0.547366</td>\n      <td>...</td>\n      <td>-0.468706</td>\n      <td>0.047521</td>\n      <td>-0.411941</td>\n      <td>0.150299</td>\n      <td>-0.348786</td>\n      <td>-0.128443</td>\n      <td>[feel, delug, low, take, quiz]</td>\n      <td>53</td>\n      <td>144</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'm drowning in spirits to wash you out</td>\n      <td>0</td>\n      <td>[drown, spirit, wash]</td>\n      <td>drown spirit wash</td>\n      <td>-0.022870</td>\n      <td>-0.046965</td>\n      <td>-0.039346</td>\n      <td>-0.056824</td>\n      <td>0.015039</td>\n      <td>-0.026105</td>\n      <td>...</td>\n      <td>-0.470581</td>\n      <td>-0.142529</td>\n      <td>-0.776593</td>\n      <td>1.499534</td>\n      <td>-0.503235</td>\n      <td>0.083692</td>\n      <td>[drown]</td>\n      <td>78</td>\n      <td>206</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lunch for the crew is made. Night night it's b...</td>\n      <td>0</td>\n      <td>[lunch, crew, made, night, night, long, day, p...</td>\n      <td>lunch crew made night night long day peac love...</td>\n      <td>-0.118431</td>\n      <td>-0.081678</td>\n      <td>-0.069096</td>\n      <td>-0.011810</td>\n      <td>-0.048462</td>\n      <td>-0.031301</td>\n      <td>...</td>\n      <td>-0.863437</td>\n      <td>-0.902139</td>\n      <td>-0.041927</td>\n      <td>-0.027119</td>\n      <td>1.128388</td>\n      <td>1.424497</td>\n      <td>[crew, made, night, night, long, day, peac, lo...</td>\n      <td>215</td>\n      <td>535</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baltimore City : I-95 NORTH AT MP 54.8 (FORT M...</td>\n      <td>1</td>\n      <td>[baltimor, citi, 95, north, mp, 54.8, fort, mc...</td>\n      <td>baltimor citi 95 north mp 54.8 fort mchenri tu...</td>\n      <td>-0.101302</td>\n      <td>-0.088488</td>\n      <td>-0.108686</td>\n      <td>-0.139330</td>\n      <td>-0.067866</td>\n      <td>-0.022211</td>\n      <td>...</td>\n      <td>-0.655261</td>\n      <td>-0.039618</td>\n      <td>0.113906</td>\n      <td>-0.637715</td>\n      <td>0.313348</td>\n      <td>0.124221</td>\n      <td>[citi, 95, north, fort, mchenri, bore, 95, nor...</td>\n      <td>294</td>\n      <td>582</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I concur. The longer you spend with your child...</td>\n      <td>0</td>\n      <td>[concur, longer, spend, child, harm, mmk]</td>\n      <td>concur longer spend child harm mmk</td>\n      <td>-0.100515</td>\n      <td>-0.066034</td>\n      <td>-0.051609</td>\n      <td>-0.019156</td>\n      <td>-0.034221</td>\n      <td>-0.026097</td>\n      <td>...</td>\n      <td>0.277074</td>\n      <td>-0.421111</td>\n      <td>0.040377</td>\n      <td>0.072370</td>\n      <td>-0.105157</td>\n      <td>-0.400602</td>\n      <td>[concur, spend, child, harm, mmk]</td>\n      <td>308</td>\n      <td>627</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6084</th>\n      <td>If I could I would have been by at work but go...</td>\n      <td>0</td>\n      <td>[could, would, work, got, injur, secur, concer...</td>\n      <td>could would work got injur secur concern must ...</td>\n      <td>-0.122435</td>\n      <td>-0.076033</td>\n      <td>-0.066928</td>\n      <td>-0.093147</td>\n      <td>0.077987</td>\n      <td>-0.098983</td>\n      <td>...</td>\n      <td>-0.331956</td>\n      <td>0.344637</td>\n      <td>-0.475023</td>\n      <td>0.285512</td>\n      <td>0.856661</td>\n      <td>-1.022422</td>\n      <td>[could, would, work, got, injur, secur, concer...</td>\n      <td>672435</td>\n      <td>874123</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6085</th>\n      <td>@UntamedDirewolf 'I... Wow. Alright.' Sansa sh...</td>\n      <td>0</td>\n      <td>[wow, alright, sansa, shook, head, blink, rapi...</td>\n      <td>wow alright sansa shook head blink rapidli new...</td>\n      <td>-0.124342</td>\n      <td>-0.096810</td>\n      <td>-0.091506</td>\n      <td>-0.067413</td>\n      <td>-0.027625</td>\n      <td>-0.052506</td>\n      <td>...</td>\n      <td>-0.360378</td>\n      <td>-0.333349</td>\n      <td>-0.049440</td>\n      <td>-0.095271</td>\n      <td>-0.206000</td>\n      <td>0.604812</td>\n      <td>[wow, alright, sansa, head, blink, rapidli, ne...</td>\n      <td>672544</td>\n      <td>874436</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6086</th>\n      <td>well it feels like im on fire.</td>\n      <td>0</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>well feel like im fire</td>\n      <td>-0.105781</td>\n      <td>-0.044915</td>\n      <td>-0.061630</td>\n      <td>0.030155</td>\n      <td>-0.021312</td>\n      <td>-0.007631</td>\n      <td>...</td>\n      <td>-0.329315</td>\n      <td>-0.263997</td>\n      <td>-0.514500</td>\n      <td>0.417821</td>\n      <td>0.474201</td>\n      <td>0.152877</td>\n      <td>[well, feel, like, im, fire]</td>\n      <td>672874</td>\n      <td>874877</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6087</th>\n      <td>We destroyed the #Zimmerman fan club on Twitte...</td>\n      <td>0</td>\n      <td>[destroy, zimmerman, fan, club, twitter, oblit...</td>\n      <td>destroy zimmerman fan club twitter obliter ren...</td>\n      <td>-0.119875</td>\n      <td>-0.094695</td>\n      <td>-0.017337</td>\n      <td>-0.098059</td>\n      <td>-0.046858</td>\n      <td>-0.042815</td>\n      <td>...</td>\n      <td>0.214931</td>\n      <td>0.369555</td>\n      <td>0.079218</td>\n      <td>-0.024805</td>\n      <td>0.057221</td>\n      <td>-0.210481</td>\n      <td>[destroy, fan, club, twitter, obliter, reduc]</td>\n      <td>672932</td>\n      <td>875021</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6088</th>\n      <td>#psd #special Olap #world pres: http://t.co/9x...</td>\n      <td>1</td>\n      <td>[psd, special, olap, world, pre, recogn, hazar...</td>\n      <td>psd special olap world pre recogn hazard wast ...</td>\n      <td>-0.113895</td>\n      <td>-0.086954</td>\n      <td>0.125022</td>\n      <td>-0.117756</td>\n      <td>0.259621</td>\n      <td>-0.051785</td>\n      <td>...</td>\n      <td>0.964625</td>\n      <td>-0.341183</td>\n      <td>-0.329364</td>\n      <td>-1.301500</td>\n      <td>-1.127608</td>\n      <td>0.540629</td>\n      <td>[psd, world, pre, hazard, multidimensi]</td>\n      <td>672973</td>\n      <td>875073</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6089 rows Ã— 3317 columns</p>\n</div>"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"print(top_words)\nlen(top_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.440091Z","iopub.execute_input":"2025-07-28T05:31:57.440406Z","iopub.status.idle":"2025-07-28T05:31:57.447177Z","shell.execute_reply.started":"2025-07-28T05:31:57.440380Z","shell.execute_reply":"2025-07-28T05:31:57.446274Z"}},"outputs":[{"name":"stdout","text":"['000' '00pm' '02' ... 'Ã»Ã¯you' 'Ã»Ã²' 'Ã»Ã³']\n","output_type":"stream"},{"execution_count":142,"output_type":"execute_result","data":{"text/plain":"5309"},"metadata":{}}],"execution_count":142},{"cell_type":"markdown","source":"# 3. Build Word Dictionary\nUse the training dataset to build the word dictionary. The word dictionary will use the ($word_i$, $label_i$) as key and the count of ($word_i$, $label_i$) occurrence as value","metadata":{}},{"cell_type":"code","source":"def build_word_dict(label_arr, token_word_arr):\n    \"\"\"Build frequencies.\n    Input:\n        token_word: a series of list of tokenized word\n        label: a series of label that match the array of the list of tokenized word\n    Output:\n        freqs: a dictionary mapping each (word, label) pair to its frequency\n    \"\"\"\n    word_dict = {}\n    y_list = list(label_arr) # make array into list\n\n    for label_idx in range(len(y_list)):\n\n        for word in token_word_arr[label_idx]:\n            word_dict[(word, y_list[label_idx])] = word_dict.get((word, y_list[label_idx]), 0) + 1\n    return word_dict\n        \nword_dict = build_word_dict(train_df_fnl['target'], train_df_fnl[\"top_words\"])    \n  \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.448209Z","iopub.execute_input":"2025-07-28T05:31:57.448558Z","iopub.status.idle":"2025-07-28T05:31:57.497085Z","shell.execute_reply.started":"2025-07-28T05:31:57.448531Z","shell.execute_reply":"2025-07-28T05:31:57.496290Z"}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"print(\"number of words:\", len(word_dict))\nprint(\"Output Example:\", list(word_dict.items())[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.499254Z","iopub.execute_input":"2025-07-28T05:31:57.499522Z","iopub.status.idle":"2025-07-28T05:31:57.505487Z","shell.execute_reply.started":"2025-07-28T05:31:57.499502Z","shell.execute_reply":"2025-07-28T05:31:57.504578Z"}},"outputs":[{"name":"stdout","text":"number of words: 7222\nOutput Example: [(('feel', 0), 64), (('delug', 0), 43), (('low', 0), 15), (('take', 0), 52), (('quiz', 0), 13), (('drown', 0), 62), (('crew', 0), 2), (('made', 0), 28), (('night', 0), 31), (('long', 0), 18)]\n","output_type":"stream"}],"execution_count":144},{"cell_type":"markdown","source":"# Extract Information","metadata":{}},{"cell_type":"markdown","source":"$$X_m = [x_0, x_1, x_2]\n    =[1, \\sum_{i=1}^{m} Freq(_i, 1), \\sum_{i=1}^{m}  Freq(w_i,0)]$$\n\n$x_0$: bias\n\n$x_1$: number of postive label for this word from the dictionary\n\n$x_2$: number of negative label for this word from the dictionary\n\n$m$: number of training tweets","metadata":{}},{"cell_type":"code","source":"def extract_info(word_dict,token_word_arr):\n    \"\"\"Extract Information from Each Tweets.\n    Input:\n        token_word_arr: a series of list of tokenized word\n        word_dic: the dictionary with (w, label) as key and frequency as value\n    Output:\n        1. a list of total frequency for each word in the tweet that associate with disaster from the dictionary\n        2. a list of total frequency for each word in the tweet that associate with not disaster from the dictionary\n    \"\"\"\n    cnt_dis_word = 0\n    cnt_non_dis_word = 0\n    dis_lst = []\n    non_dis_lst = []\n\n    # iterate all rows\n    for idx in range(len(token_word_arr)):\n        # iterate all words at the token_word_arr[idx]\n        for word in token_word_arr[idx]:\n            cnt_dis_word += word_dict.get((word, 1), 0)\n            cnt_non_dis_word += word_dict.get((word, 0), 0)\n\n        dis_lst.append(cnt_dis_word)\n        non_dis_lst.append(cnt_non_dis_word)\n    return dis_lst, non_dis_lst\n    \n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.506170Z","iopub.execute_input":"2025-07-28T05:31:57.506393Z","iopub.status.idle":"2025-07-28T05:31:57.524405Z","shell.execute_reply.started":"2025-07-28T05:31:57.506376Z","shell.execute_reply":"2025-07-28T05:31:57.523514Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"# Train Dataset\ntrain_df_fnl['disaster_score'], train_df_fnl['non_disaster_score']= extract_info(word_dict,train_df_fnl[\"top_words\"])\ntrain_df_fnl['bias'] = [1]*len(train_df_fnl) # add bias term\nX_train, y_train = train_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']], train_df_fnl['target']\n\n# Validation Dataset\nval_df_fnl['disaster_score'], val_df_fnl['non_disaster_score']= extract_info(word_dict,val_df_fnl['top_words'])\nval_df_fnl['bias'] = [1]*len(val_df_fnl) # add bias term\nX_val, y_val = val_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']], val_df_fnl['target']\n\n# Test Dataset\ntest_df_fnl['disaster_score'], test_df_fnl['non_disaster_score']= extract_info(word_dict,test_df_fnl['top_words'])\ntest_df_fnl['bias'] = [1]*len(test_df_fnl) # add bias term\nX_test = test_df_fnl[[\"bias\", \"disaster_score\", 'non_disaster_score']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.525509Z","iopub.execute_input":"2025-07-28T05:31:57.525954Z","iopub.status.idle":"2025-07-28T05:31:57.623726Z","shell.execute_reply.started":"2025-07-28T05:31:57.525923Z","shell.execute_reply":"2025-07-28T05:31:57.622997Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"print(X_train.shape) #(m, 3)\nprint(X_val.shape) #(m, 3)\n\n\ny_train = y_train.to_numpy().reshape(-1, 1)\ny_val = y_val.to_numpy().reshape(-1, 1)\nprint(y_train.shape) #(m,1)\nprint(y_val.shape) # (m, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.625018Z","iopub.execute_input":"2025-07-28T05:31:57.625422Z","iopub.status.idle":"2025-07-28T05:31:57.630497Z","shell.execute_reply.started":"2025-07-28T05:31:57.625388Z","shell.execute_reply":"2025-07-28T05:31:57.629584Z"}},"outputs":[{"name":"stdout","text":"(6089, 3)\n(1524, 3)\n(6089, 1)\n(1524, 1)\n","output_type":"stream"}],"execution_count":147},{"cell_type":"markdown","source":"# Overfitting\nTo avoid overfitting, I plan to implement the L2 Regression\n$$\\lambda*\\frac{1}{2}\\mathbf{\\theta}^T\\mathbf{\\theta}$$","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"* Sigmoid Function\n$$ h(z) = \\frac{1}{1+\\exp^{-z}} $$\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N = x^Tz$$\n\n* Loss Function\n$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n    * By Taking Derivative of Log of Likelihood: $$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j $$\n* Loss Function with Ridge Regression\n$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)+\\frac{\\lambda}{2m}\\mathbf{\\theta}^T\\mathbf{\\theta}$$\n    * By Taking Derivative of Log of Likelihood: $$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} (\\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j + \\lambda \\theta_j )$$\n* Gradient Descent\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n\n$\\alpha$: learning rate\n\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"Calculate sigmoid.\n    Input: a digit\n    Output: the sigmoid of the digit\n    \"\"\"   \n    z = np.clip(z, -500, 500)  # Prevent overflow\n    return 1/(1+np.exp(-z))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.631297Z","iopub.execute_input":"2025-07-28T05:31:57.631592Z","iopub.status.idle":"2025-07-28T05:31:57.649981Z","shell.execute_reply.started":"2025-07-28T05:31:57.631565Z","shell.execute_reply":"2025-07-28T05:31:57.648933Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"def accuracy(pred_label, label_y):\n    \"\"\"Calculate accuracy.\n    Input: predicted Y and actual Y value\n    Output: a list contains 0 or 1 if the predicted Y is the same as label_y\n    \"\"\"  \n    match_list = (pred_label == label_y).astype(int)\n    accuracy_percentage = sum(match_list)/ len(match_list)\n    return accuracy_percentage\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.650923Z","iopub.execute_input":"2025-07-28T05:31:57.651203Z","iopub.status.idle":"2025-07-28T05:31:57.666098Z","shell.execute_reply.started":"2025-07-28T05:31:57.651183Z","shell.execute_reply":"2025-07-28T05:31:57.665166Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"def gradDescent(alpha, theta, train_X, train_Y, val_X, val_Y,iteration, lam):\n    \"\"\"Calculate sigmoid.\n    Input: a digit\n    Output: the sigmoid of the digit\n    \"\"\"   \n    # initialize the train and validation loss list, also the theta\n    train_loss = []\n    val_loss = []\n    train_accuracy = []\n    val_accuracy = []\n    theta = np.zeros((3,1))\n\n    # optimal_iter = 0\n\n    # number of examples\n    train_m = len(train_Y)  \n    val_m = len(val_Y)\n    \n    \n    for itera in range(iteration):\n\n        # calculate the predicted y\n        z = np.dot(train_X,theta) #(m,1)\n\n        # put the predicted y into sigmoid\n        sig = sigmoid(z) #(m,1)\n\n        # Compute the gradient\n        gradient = (1/train_m) * np.dot(train_X.T, (sig - train_Y)) #(3,1)\n        gradient += (lam / train_m) * np.r_[[[0]], theta[1:]]  # exclude bias\n\n\n        # update theta\n        theta = theta - gradient * alpha #(3,1)\n\n\n        # calculate the training loss\n        train_z = np.dot(train_X,theta)        \n        train_sig = sigmoid(train_z)\n        train_sig = np.clip(train_sig, 1e-10, 1 - 1e-10)  # Prevent log(0)\n        train_loss_val = (-1/train_m)* (train_Y.T @ np.log(train_sig)+(1-train_Y).T@ np.log(1-train_sig))[0,0]+(lam / (2 * train_m)) * np.sum(theta[1:] ** 2)\n        train_loss.append(train_loss_val)\n\n        # predict the training label\n        pred_label_train = (train_sig >= 0.5).astype(int)\n        # calculate the training accuray\n        train_accuracy.append(accuracy_score(pred_label_train,train_Y))\n        \n        \n        # calculate the validation loss\n        val_z = np.dot(val_X,theta)\n        val_sig = sigmoid(val_z)\n        val_sig = np.clip(val_sig, 1e-10, 1 - 1e-10)  # Prevent log(0)\n        val_loss_val = (-1/val_m)* (val_Y.T @ np.log(val_sig)+(1-val_Y).T@ np.log(1-val_sig))[0,0]\n        val_loss.append(val_loss_val)   \n        \n        # predict the validation label\n        pred_label_val = (val_sig  >= 0.5).astype(int)\n        # calculate the training accuray\n        val_accuracy.append(accuracy_score(pred_label_val,val_Y))\n        \n\n\n        \n        # # update the theta optimal theta\n\n        # curr_train_val_loss = np.log(train_loss_val) + np.log(val_loss_val)\n        \n        # if curr_train_val_loss < min_train_val_loss:\n        #     theta_optimal = theta\n        #     min_train_val_loss = curr_train_val_loss\n        #     optimal_iter = itera\n\n    \n    return theta, train_loss, val_loss, train_accuracy, val_accuracy\n    \n    \n    \n    \n    \n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.667007Z","iopub.execute_input":"2025-07-28T05:31:57.667295Z","iopub.status.idle":"2025-07-28T05:31:57.678806Z","shell.execute_reply.started":"2025-07-28T05:31:57.667268Z","shell.execute_reply":"2025-07-28T05:31:57.678124Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"# y_axis[int(5500/500)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.681195Z","iopub.execute_input":"2025-07-28T05:31:57.681487Z","iopub.status.idle":"2025-07-28T05:31:57.700320Z","shell.execute_reply.started":"2025-07-28T05:31:57.681458Z","shell.execute_reply":"2025-07-28T05:31:57.699310Z"}},"outputs":[],"execution_count":151},{"cell_type":"code","source":"theta = np.zeros((3,1))\nalpha = 0.0001\niteration = 1000\nlam = 0.0001\ntheta, train_loss, val_loss, train_accuracy, val_accuracy = gradDescent(alpha,theta,X_train,y_train, X_val, y_val,iteration, lam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:31:57.702691Z","iopub.execute_input":"2025-07-28T05:31:57.703093Z","iopub.status.idle":"2025-07-28T05:32:00.886718Z","shell.execute_reply.started":"2025-07-28T05:31:57.703068Z","shell.execute_reply":"2025-07-28T05:32:00.886005Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"print(theta)\nprint(np.mean(train_accuracy))\nprint(np.mean(val_accuracy))\nprint(train_accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:32:00.888213Z","iopub.execute_input":"2025-07-28T05:32:00.888916Z","iopub.status.idle":"2025-07-28T05:32:00.895527Z","shell.execute_reply.started":"2025-07-28T05:32:00.888886Z","shell.execute_reply":"2025-07-28T05:32:00.894642Z"}},"outputs":[{"name":"stdout","text":"[[ 6.13151187e-04]\n [-1.19130021e+01]\n [-1.49716959e+01]]\n0.5108397109541796\n0.5108405511811024\n[0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570701264575464, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570701264575464, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42979142716373786, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.4301198883232058, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.42962719658400395, 0.570372803415996, 0.570372803415996, 0.42962719658400395, 0.570372803415996]\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# iterations = range(1, iteration + 1)\n# plt.figure(figsize=(20, 5))\n# plt.plot(iterations, train_accuracy, label='Train Accuracy', color='blue')\n# plt.plot(iterations, val_accuracy, label='Validation Accuracy', color='orange')\n# plt.xlabel('Iteration')\n# plt.ylabel('Accuracy')\n# plt.title('Train vs Test Loss over Iterations')\n# plt.legend()\n# plt.grid(True)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T05:32:00.896500Z","iopub.execute_input":"2025-07-28T05:32:00.896858Z","iopub.status.idle":"2025-07-28T05:32:00.914247Z","shell.execute_reply.started":"2025-07-28T05:32:00.896838Z","shell.execute_reply":"2025-07-28T05:32:00.913470Z"}},"outputs":[],"execution_count":154},{"cell_type":"markdown","source":"Reference: [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)","metadata":{}}]}